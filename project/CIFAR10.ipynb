{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "\n",
    "from fuel.transformers import ScaleAndShift, Cast\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "\n",
    "from blocks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Theano to debug mode.\n"
     ]
    }
   ],
   "source": [
    "### Generic config\n",
    "\n",
    "Cfg_Data_BatchSize_Train    = 20\n",
    "Cfg_Data_BatchSize_Validate = 100\n",
    "Cfg_Data_BatchSize_Test     = 100\n",
    "\n",
    "Cfg_Theano_Debug = True\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "if Cfg_Theano_Debug:\n",
    "    print \"Set Theano to debug mode.\"\n",
    "    theano.config.optimizer = 'fast_compile'\n",
    "    theano.config.exception_verbosity = 'high'\n",
    "else:\n",
    "    theano.config.optimizer = 'fast_run'\n",
    "    theano.config.exception_verbosity = 'low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input data.\n",
      "Subset sizes: Train 45000, Validation 5000, Test 10000\n",
      "Stream data shapes:\n",
      "Stream_Train: image batch of shape (20, 3, 32, 32) [float32], and label batch of shape (20, 1) [uint8]\n",
      "Stream_Test: image batch of shape (100, 3, 32, 32) [float32], and label batch of shape (100, 1) [uint8]\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'})\n",
    ")\n",
    "\n",
    "Data_Train      = CIFAR10((\"train\",), subset=slice(None ,45000))\n",
    "Data_Validation = CIFAR10((\"train\",), subset=slice(45000, None))\n",
    "Data_Test       = CIFAR10((\"test\" ,)                           )\n",
    "\n",
    "Stream_Train     = DataStream.default_stream(\n",
    "                      Data_Train,\n",
    "                      iteration_scheme=ShuffledScheme(Data_Train.num_examples       , Cfg_Data_BatchSize_Train)\n",
    "                   )\n",
    "Stream_Validation = DataStream.default_stream(\n",
    "                      Data_Validation,\n",
    "                      iteration_scheme=SequentialScheme(Data_Validation.num_examples, Cfg_Data_BatchSize_Validate)\n",
    "                   )\n",
    "Stream_Test       = DataStream.default_stream(\n",
    "                      Data_Test,\n",
    "                      iteration_scheme=SequentialScheme(Data_Test.num_examples      , Cfg_Data_BatchSize_Test)\n",
    "                   )\n",
    "\n",
    "print \"Loaded input data.\"\n",
    "print \"Subset sizes: Train %d, Validation %d, Test %d\" % (Data_Train.num_examples, Data_Validation.num_examples, Data_Test.num_examples)\n",
    "\n",
    "def GetNextBatch(stream):\n",
    "    return next(stream.get_epoch_iterator())\n",
    "\n",
    "print \"Stream data shapes:\"\n",
    "x,y = GetNextBatch(Stream_Train)\n",
    "print \"Stream_Train: image batch of shape %s [%s], and label batch of shape %s [%s]\" % (x.shape, x.dtype, y.shape, y.dtype)\n",
    "x,y = GetNextBatch(Stream_Test)\n",
    "print \"Stream_Test: image batch of shape %s [%s], and label batch of shape %s [%s]\" % (x.shape, x.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4('X')\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "X_test_value, Y_test_value = GetNextBatch(Stream_Train)\n",
    "theano.config.compute_test_value = 'off' # Disable the computation of test values\n",
    "\n",
    "X.tag.test_value = X_test_value[:3]\n",
    "Y.tag.test_value = Y_test_value[:3]\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "#The first convolutional layer\n",
    "#The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 10 #we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1,3,5,5), dtype='float32'),\n",
    "                   name='CW1')\n",
    "#please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype='float32'),\n",
    "                    name='CB1')\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(X, CW1, filter_shape=(5,5)) + CB1.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2,2), ignore_border=True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_filters_2 = 25 #we will compute ten convolution filters in the first layer\n",
    "CW2 = theano.shared(np.zeros((num_filters_2,num_filters_1,5,5), dtype='float32'),\n",
    "                   name='CW2')\n",
    "CW2.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype='float32'),\n",
    "                    name='CB2')\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2,2), ignore_border=True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "#Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden=500\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype='float32'),\n",
    "                   name='FW3')\n",
    "FW3.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype='float32'),\n",
    "                    name='FB3')\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle('x',0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_fw4_hidden=10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype='float32'),\n",
    "                   name='FW4')\n",
    "FW4.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype='float32'),\n",
    "                    name='FB4')\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle('x',0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "#At this point stop computing test values\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []\n",
    "\n",
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "\n",
    "for p,g,v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n",
    "\n",
    "def init_parameters():\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.703885, batch nll 1.900320, batch error rate 75.000000%\n",
      "At minibatch 200, batch loss 2.692368, batch nll 1.899582, batch error rate 80.000000%\n",
      "At minibatch 300, batch loss 2.430619, batch nll 1.647972, batch error rate 45.000000%\n",
      "At minibatch 400, batch loss 2.370766, batch nll 1.598010, batch error rate 60.000000%\n",
      "At minibatch 500, batch loss 2.491653, batch nll 1.728923, batch error rate 45.000000%\n",
      "At minibatch 600, batch loss 2.298749, batch nll 1.545995, batch error rate 50.000000%\n",
      "At minibatch 700, batch loss 1.807559, batch nll 1.064499, batch error rate 35.000000%\n",
      "At minibatch 800, batch loss 2.504328, batch nll 1.771201, batch error rate 50.000000%\n",
      "At minibatch 900, batch loss 2.228119, batch nll 1.504578, batch error rate 65.000000%\n",
      "At minibatch 1000, batch loss 2.714745, batch nll 2.000682, batch error rate 65.000000%\n",
      "At minibatch 1100, batch loss 2.408517, batch nll 1.703583, batch error rate 50.000000%\n",
      "At minibatch 1200, batch loss 1.939713, batch nll 1.243694, batch error rate 55.000000%\n",
      "At minibatch 1300, batch loss 2.249674, batch nll 1.562712, batch error rate 60.000000%\n",
      "At minibatch 1400, batch loss 1.958521, batch nll 1.280077, batch error rate 50.000000%\n",
      "At minibatch 1500, batch loss 2.309654, batch nll 1.639728, batch error rate 60.000000%\n",
      "At minibatch 1600, batch loss 1.941718, batch nll 1.280547, batch error rate 55.000000%\n",
      "At minibatch 1700, batch loss 2.085152, batch nll 1.431713, batch error rate 50.000000%\n",
      "At minibatch 1800, batch loss 1.877434, batch nll 1.232335, batch error rate 40.000000%\n",
      "At minibatch 1900, batch loss 1.945345, batch nll 1.308077, batch error rate 50.000000%\n",
      "At minibatch 2000, batch loss 1.911440, batch nll 1.281600, batch error rate 45.000000%\n",
      "At minibatch 2100, batch loss 1.987283, batch nll 1.364857, batch error rate 50.000000%\n",
      "At minibatch 2200, batch loss 2.051285, batch nll 1.436271, batch error rate 50.000000%\n",
      "After epoch 1: valid_err_rate: 44.980000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 55.560000% averaged train nll: 1.535334 averaged train loss: 2.241417\n",
      "At minibatch 2300, batch loss 2.135690, batch nll 1.526551, batch error rate 55.000000%\n",
      "At minibatch 2400, batch loss 1.939854, batch nll 1.336501, batch error rate 50.000000%\n",
      "At minibatch 2500, batch loss 2.067621, batch nll 1.469649, batch error rate 60.000000%\n",
      "At minibatch 2600, batch loss 1.800167, batch nll 1.207784, batch error rate 45.000000%\n",
      "At minibatch 2700, batch loss 1.875829, batch nll 1.288872, batch error rate 50.000000%\n",
      "At minibatch 2800, batch loss 1.408470, batch nll 0.826291, batch error rate 30.000000%\n",
      "At minibatch 2900, batch loss 1.993060, batch nll 1.415605, batch error rate 45.000000%\n",
      "At minibatch 3000, batch loss 1.890547, batch nll 1.317909, batch error rate 45.000000%\n",
      "At minibatch 3100, batch loss 1.668540, batch nll 1.100191, batch error rate 50.000000%\n",
      "At minibatch 3200, batch loss 1.756746, batch nll 1.192396, batch error rate 45.000000%\n",
      "At minibatch 3300, batch loss 1.779787, batch nll 1.219445, batch error rate 50.000000%\n",
      "At minibatch 3400, batch loss 1.583886, batch nll 1.027728, batch error rate 35.000000%\n",
      "At minibatch 3500, batch loss 1.465297, batch nll 0.912695, batch error rate 25.000000%\n",
      "At minibatch 3600, batch loss 1.484628, batch nll 0.935227, batch error rate 35.000000%\n",
      "At minibatch 3700, batch loss 1.290708, batch nll 0.744880, batch error rate 20.000000%\n",
      "At minibatch 3800, batch loss 1.682262, batch nll 1.139911, batch error rate 25.000000%\n",
      "At minibatch 3900, batch loss 1.255936, batch nll 0.716725, batch error rate 25.000000%\n",
      "At minibatch 4000, batch loss 1.767107, batch nll 1.231323, batch error rate 50.000000%\n",
      "At minibatch 4100, batch loss 1.717834, batch nll 1.184837, batch error rate 40.000000%\n",
      "At minibatch 4200, batch loss 1.492098, batch nll 0.962196, batch error rate 40.000000%\n",
      "At minibatch 4300, batch loss 1.624326, batch nll 1.097023, batch error rate 35.000000%\n",
      "At minibatch 4400, batch loss 1.780749, batch nll 1.256462, batch error rate 50.000000%\n",
      "At minibatch 4500, batch loss 1.353366, batch nll 0.831478, batch error rate 35.000000%\n",
      "After epoch 2: valid_err_rate: 39.080000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 41.648889% averaged train nll: 1.177575 averaged train loss: 1.738108\n",
      "At minibatch 4600, batch loss 1.444399, batch nll 0.924664, batch error rate 30.000000%\n",
      "At minibatch 4700, batch loss 1.715473, batch nll 1.197846, batch error rate 50.000000%\n",
      "At minibatch 4800, batch loss 1.940182, batch nll 1.424665, batch error rate 30.000000%\n",
      "At minibatch 4900, batch loss 1.611271, batch nll 1.097812, batch error rate 35.000000%\n",
      "At minibatch 5000, batch loss 1.950421, batch nll 1.439407, batch error rate 50.000000%\n",
      "At minibatch 5100, batch loss 1.731749, batch nll 1.222531, batch error rate 30.000000%\n",
      "At minibatch 5200, batch loss 1.442127, batch nll 0.934977, batch error rate 35.000000%\n",
      "At minibatch 5300, batch loss 1.796754, batch nll 1.291618, batch error rate 40.000000%\n",
      "At minibatch 5400, batch loss 1.561976, batch nll 1.058574, batch error rate 40.000000%\n",
      "At minibatch 5500, batch loss 1.781606, batch nll 1.279925, batch error rate 50.000000%\n",
      "At minibatch 5600, batch loss 1.526726, batch nll 1.026894, batch error rate 35.000000%\n",
      "At minibatch 5700, batch loss 1.412298, batch nll 0.914406, batch error rate 35.000000%\n",
      "At minibatch 5800, batch loss 1.326031, batch nll 0.829851, batch error rate 35.000000%\n",
      "At minibatch 5900, batch loss 1.289791, batch nll 0.795332, batch error rate 30.000000%\n",
      "At minibatch 6000, batch loss 1.584617, batch nll 1.091861, batch error rate 30.000000%\n",
      "At minibatch 6100, batch loss 1.914310, batch nll 1.423123, batch error rate 60.000000%\n",
      "At minibatch 6200, batch loss 1.241975, batch nll 0.752794, batch error rate 40.000000%\n",
      "At minibatch 6300, batch loss 1.384171, batch nll 0.896412, batch error rate 40.000000%\n",
      "At minibatch 6400, batch loss 1.955158, batch nll 1.468798, batch error rate 45.000000%\n",
      "At minibatch 6500, batch loss 1.795182, batch nll 1.310452, batch error rate 45.000000%\n",
      "At minibatch 6600, batch loss 1.511072, batch nll 1.027744, batch error rate 30.000000%\n",
      "At minibatch 6700, batch loss 1.803963, batch nll 1.322073, batch error rate 55.000000%\n",
      "After epoch 3: valid_err_rate: 37.340000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 35.228889% averaged train nll: 1.007107 averaged train loss: 1.507106\n",
      "At minibatch 6800, batch loss 1.246119, batch nll 0.765723, batch error rate 30.000000%\n",
      "At minibatch 6900, batch loss 1.052001, batch nll 0.572559, batch error rate 15.000000%\n",
      "At minibatch 7000, batch loss 1.550102, batch nll 1.071903, batch error rate 35.000000%\n",
      "At minibatch 7100, batch loss 1.341436, batch nll 0.864448, batch error rate 30.000000%\n",
      "At minibatch 7200, batch loss 1.941993, batch nll 1.466044, batch error rate 55.000000%\n",
      "At minibatch 7300, batch loss 1.248341, batch nll 0.773514, batch error rate 25.000000%\n",
      "At minibatch 7400, batch loss 1.292068, batch nll 0.818334, batch error rate 35.000000%\n",
      "At minibatch 7500, batch loss 1.170145, batch nll 0.697653, batch error rate 30.000000%\n",
      "At minibatch 7600, batch loss 1.665412, batch nll 1.194195, batch error rate 40.000000%\n",
      "At minibatch 7700, batch loss 1.399037, batch nll 0.928899, batch error rate 25.000000%\n",
      "At minibatch 7800, batch loss 1.617341, batch nll 1.148151, batch error rate 40.000000%\n",
      "At minibatch 7900, batch loss 1.500697, batch nll 1.032836, batch error rate 25.000000%\n",
      "At minibatch 8000, batch loss 1.508187, batch nll 1.041134, batch error rate 40.000000%\n",
      "At minibatch 8100, batch loss 1.569770, batch nll 1.103983, batch error rate 40.000000%\n",
      "At minibatch 8200, batch loss 1.080099, batch nll 0.615207, batch error rate 15.000000%\n",
      "At minibatch 8300, batch loss 1.111495, batch nll 0.647488, batch error rate 25.000000%\n",
      "At minibatch 8400, batch loss 1.327110, batch nll 0.864263, batch error rate 30.000000%\n",
      "At minibatch 8500, batch loss 1.246846, batch nll 0.784924, batch error rate 30.000000%\n",
      "At minibatch 8600, batch loss 1.656187, batch nll 1.195383, batch error rate 35.000000%\n",
      "At minibatch 8700, batch loss 1.291311, batch nll 0.831716, batch error rate 25.000000%\n",
      "At minibatch 8800, batch loss 0.915071, batch nll 0.456407, batch error rate 10.000000%\n",
      "At minibatch 8900, batch loss 1.247632, batch nll 0.789977, batch error rate 35.000000%\n",
      "At minibatch 9000, batch loss 1.152148, batch nll 0.695460, batch error rate 15.000000%\n",
      "After epoch 4: valid_err_rate: 33.820000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 31.475556% averaged train nll: 0.901304 averaged train loss: 1.369844\n",
      "At minibatch 9100, batch loss 1.496335, batch nll 1.040316, batch error rate 30.000000%\n",
      "At minibatch 9200, batch loss 1.077549, batch nll 0.622428, batch error rate 20.000000%\n",
      "At minibatch 9300, batch loss 1.398201, batch nll 0.943802, batch error rate 40.000000%\n",
      "At minibatch 9400, batch loss 1.257112, batch nll 0.803380, batch error rate 25.000000%\n",
      "At minibatch 9500, batch loss 1.559846, batch nll 1.106879, batch error rate 40.000000%\n",
      "At minibatch 9600, batch loss 1.517038, batch nll 1.064908, batch error rate 30.000000%\n",
      "At minibatch 9700, batch loss 1.563215, batch nll 1.111815, batch error rate 25.000000%\n",
      "At minibatch 9800, batch loss 1.587075, batch nll 1.136333, batch error rate 30.000000%\n",
      "At minibatch 9900, batch loss 1.349667, batch nll 0.899544, batch error rate 35.000000%\n",
      "At minibatch 10000, batch loss 1.490202, batch nll 1.040824, batch error rate 35.000000%\n",
      "At minibatch 10100, batch loss 1.382184, batch nll 0.933515, batch error rate 40.000000%\n",
      "At minibatch 10200, batch loss 1.802885, batch nll 1.354859, batch error rate 60.000000%\n",
      "At minibatch 10300, batch loss 1.164962, batch nll 0.717902, batch error rate 30.000000%\n",
      "At minibatch 10400, batch loss 1.607507, batch nll 1.161111, batch error rate 50.000000%\n",
      "At minibatch 10500, batch loss 0.965693, batch nll 0.520044, batch error rate 15.000000%\n",
      "At minibatch 10600, batch loss 1.458411, batch nll 1.013430, batch error rate 30.000000%\n",
      "At minibatch 10700, batch loss 1.312109, batch nll 0.867927, batch error rate 25.000000%\n",
      "At minibatch 10800, batch loss 1.496563, batch nll 1.053243, batch error rate 40.000000%\n",
      "At minibatch 10900, batch loss 1.030039, batch nll 0.587493, batch error rate 25.000000%\n",
      "At minibatch 11000, batch loss 1.210919, batch nll 0.768931, batch error rate 35.000000%\n",
      "At minibatch 11100, batch loss 1.235358, batch nll 0.794049, batch error rate 30.000000%\n",
      "At minibatch 11200, batch loss 1.277601, batch nll 0.836876, batch error rate 30.000000%\n",
      "After epoch 5: valid_err_rate: 32.140000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 28.615556% averaged train nll: 0.826296 averaged train loss: 1.274694\n",
      "At minibatch 11300, batch loss 1.157410, batch nll 0.717310, batch error rate 30.000000%\n",
      "At minibatch 11400, batch loss 1.671351, batch nll 1.231660, batch error rate 60.000000%\n",
      "At minibatch 11500, batch loss 1.441718, batch nll 1.002599, batch error rate 35.000000%\n",
      "At minibatch 11600, batch loss 1.061226, batch nll 0.622655, batch error rate 20.000000%\n",
      "At minibatch 11700, batch loss 1.204643, batch nll 0.766547, batch error rate 30.000000%\n",
      "At minibatch 11800, batch loss 1.104182, batch nll 0.666573, batch error rate 30.000000%\n",
      "At minibatch 11900, batch loss 0.875325, batch nll 0.438168, batch error rate 10.000000%\n",
      "At minibatch 12000, batch loss 1.616208, batch nll 1.179633, batch error rate 35.000000%\n",
      "At minibatch 12100, batch loss 1.023453, batch nll 0.587358, batch error rate 15.000000%\n",
      "At minibatch 12200, batch loss 1.257395, batch nll 0.821807, batch error rate 35.000000%\n",
      "At minibatch 12300, batch loss 1.471161, batch nll 1.036234, batch error rate 30.000000%\n",
      "At minibatch 12400, batch loss 1.111618, batch nll 0.677144, batch error rate 25.000000%\n",
      "At minibatch 12500, batch loss 1.431015, batch nll 0.997047, batch error rate 25.000000%\n",
      "At minibatch 12600, batch loss 1.031038, batch nll 0.597567, batch error rate 15.000000%\n",
      "At minibatch 12700, batch loss 1.015557, batch nll 0.582661, batch error rate 25.000000%\n",
      "At minibatch 12800, batch loss 1.170827, batch nll 0.738424, batch error rate 25.000000%\n",
      "At minibatch 12900, batch loss 0.991469, batch nll 0.559607, batch error rate 20.000000%\n",
      "At minibatch 13000, batch loss 1.387307, batch nll 0.955888, batch error rate 35.000000%\n",
      "At minibatch 13100, batch loss 1.554209, batch nll 1.123302, batch error rate 40.000000%\n",
      "At minibatch 13200, batch loss 1.185907, batch nll 0.755411, batch error rate 20.000000%\n",
      "At minibatch 13300, batch loss 1.072017, batch nll 0.642110, batch error rate 30.000000%\n",
      "At minibatch 13400, batch loss 0.949409, batch nll 0.520054, batch error rate 25.000000%\n",
      "At minibatch 13500, batch loss 1.435153, batch nll 1.006373, batch error rate 30.000000%\n",
      "After epoch 6: valid_err_rate: 30.480000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 26.493333% averaged train nll: 0.768941 averaged train loss: 1.203570\n",
      "At minibatch 13600, batch loss 0.897983, batch nll 0.469472, batch error rate 15.000000%\n",
      "At minibatch 13700, batch loss 1.200890, batch nll 0.772668, batch error rate 20.000000%\n",
      "At minibatch 13800, batch loss 1.059753, batch nll 0.631881, batch error rate 30.000000%\n",
      "At minibatch 13900, batch loss 1.290564, batch nll 0.863073, batch error rate 25.000000%\n",
      "At minibatch 14000, batch loss 1.094902, batch nll 0.667788, batch error rate 25.000000%\n",
      "At minibatch 14100, batch loss 1.689560, batch nll 1.262887, batch error rate 50.000000%\n",
      "At minibatch 14200, batch loss 1.092242, batch nll 0.665917, batch error rate 25.000000%\n",
      "At minibatch 14300, batch loss 1.105915, batch nll 0.679990, batch error rate 20.000000%\n",
      "At minibatch 14400, batch loss 1.103098, batch nll 0.677530, batch error rate 25.000000%\n",
      "At minibatch 14500, batch loss 1.035605, batch nll 0.610361, batch error rate 25.000000%\n",
      "At minibatch 14600, batch loss 1.261388, batch nll 0.836510, batch error rate 30.000000%\n",
      "At minibatch 14700, batch loss 1.332954, batch nll 0.908473, batch error rate 35.000000%\n",
      "At minibatch 14800, batch loss 1.354754, batch nll 0.930711, batch error rate 40.000000%\n",
      "At minibatch 14900, batch loss 0.944944, batch nll 0.521388, batch error rate 15.000000%\n",
      "At minibatch 15000, batch loss 1.094062, batch nll 0.670792, batch error rate 20.000000%\n",
      "At minibatch 15100, batch loss 0.974478, batch nll 0.551594, batch error rate 15.000000%\n",
      "At minibatch 15200, batch loss 1.036518, batch nll 0.614130, batch error rate 25.000000%\n",
      "At minibatch 15300, batch loss 1.392933, batch nll 0.970852, batch error rate 35.000000%\n",
      "At minibatch 15400, batch loss 1.093030, batch nll 0.671361, batch error rate 20.000000%\n",
      "At minibatch 15500, batch loss 1.152738, batch nll 0.731502, batch error rate 25.000000%\n",
      "At minibatch 15600, batch loss 1.074467, batch nll 0.653629, batch error rate 15.000000%\n",
      "At minibatch 15700, batch loss 0.918774, batch nll 0.498305, batch error rate 10.000000%\n",
      "After epoch 7: valid_err_rate: 30.020000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 24.737778% averaged train nll: 0.719402 averaged train loss: 1.144074\n",
      "At minibatch 15800, batch loss 1.184085, batch nll 0.763963, batch error rate 30.000000%\n",
      "At minibatch 15900, batch loss 0.990764, batch nll 0.570897, batch error rate 25.000000%\n",
      "At minibatch 16000, batch loss 1.079471, batch nll 0.659852, batch error rate 30.000000%\n",
      "At minibatch 16100, batch loss 1.230958, batch nll 0.811532, batch error rate 20.000000%\n",
      "At minibatch 16200, batch loss 0.817565, batch nll 0.398462, batch error rate 10.000000%\n",
      "At minibatch 16300, batch loss 0.921638, batch nll 0.502950, batch error rate 20.000000%\n",
      "At minibatch 16400, batch loss 1.246780, batch nll 0.828408, batch error rate 20.000000%\n",
      "At minibatch 16500, batch loss 1.058577, batch nll 0.640464, batch error rate 15.000000%\n",
      "At minibatch 16600, batch loss 0.851898, batch nll 0.434090, batch error rate 15.000000%\n",
      "At minibatch 16700, batch loss 1.158344, batch nll 0.740758, batch error rate 40.000000%\n",
      "At minibatch 16800, batch loss 1.361000, batch nll 0.943711, batch error rate 30.000000%\n",
      "At minibatch 16900, batch loss 1.231523, batch nll 0.814507, batch error rate 25.000000%\n",
      "At minibatch 17000, batch loss 0.826663, batch nll 0.409973, batch error rate 15.000000%\n",
      "At minibatch 17100, batch loss 1.250293, batch nll 0.833870, batch error rate 30.000000%\n",
      "At minibatch 17200, batch loss 0.896595, batch nll 0.480511, batch error rate 20.000000%\n",
      "At minibatch 17300, batch loss 1.065065, batch nll 0.649317, batch error rate 20.000000%\n",
      "At minibatch 17400, batch loss 0.803225, batch nll 0.387717, batch error rate 15.000000%\n",
      "At minibatch 17500, batch loss 1.708041, batch nll 1.292791, batch error rate 40.000000%\n",
      "At minibatch 17600, batch loss 0.788473, batch nll 0.373552, batch error rate 10.000000%\n",
      "At minibatch 17700, batch loss 1.024986, batch nll 0.610464, batch error rate 30.000000%\n",
      "At minibatch 17800, batch loss 1.311499, batch nll 0.897229, batch error rate 35.000000%\n",
      "At minibatch 17900, batch loss 0.828800, batch nll 0.414878, batch error rate 10.000000%\n",
      "At minibatch 18000, batch loss 1.144920, batch nll 0.731320, batch error rate 25.000000%\n",
      "After epoch 8: valid_err_rate: 29.820000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 23.011111% averaged train nll: 0.675943 averaged train loss: 1.092975\n",
      "At minibatch 18100, batch loss 1.135381, batch nll 0.721984, batch error rate 20.000000%\n",
      "At minibatch 18200, batch loss 0.818092, batch nll 0.404905, batch error rate 15.000000%\n",
      "At minibatch 18300, batch loss 0.830618, batch nll 0.417675, batch error rate 15.000000%\n",
      "At minibatch 18400, batch loss 0.815739, batch nll 0.402971, batch error rate 15.000000%\n",
      "At minibatch 18500, batch loss 0.842540, batch nll 0.429964, batch error rate 10.000000%\n",
      "At minibatch 18600, batch loss 1.052213, batch nll 0.639889, batch error rate 30.000000%\n",
      "At minibatch 18700, batch loss 0.887179, batch nll 0.475132, batch error rate 20.000000%\n",
      "At minibatch 18800, batch loss 0.830587, batch nll 0.418751, batch error rate 20.000000%\n",
      "At minibatch 18900, batch loss 0.710756, batch nll 0.299106, batch error rate 5.000000%\n",
      "At minibatch 19000, batch loss 0.647641, batch nll 0.236145, batch error rate 0.000000%\n",
      "At minibatch 19100, batch loss 1.436159, batch nll 1.024969, batch error rate 35.000000%\n",
      "At minibatch 19200, batch loss 1.066992, batch nll 0.655982, batch error rate 25.000000%\n",
      "At minibatch 19300, batch loss 1.225157, batch nll 0.814443, batch error rate 25.000000%\n",
      "At minibatch 19400, batch loss 1.026904, batch nll 0.616388, batch error rate 20.000000%\n",
      "At minibatch 19500, batch loss 1.072405, batch nll 0.662129, batch error rate 20.000000%\n",
      "At minibatch 19600, batch loss 1.091275, batch nll 0.681283, batch error rate 40.000000%\n",
      "At minibatch 19700, batch loss 1.249096, batch nll 0.839309, batch error rate 15.000000%\n",
      "At minibatch 19800, batch loss 0.982415, batch nll 0.572897, batch error rate 20.000000%\n",
      "At minibatch 19900, batch loss 0.920699, batch nll 0.511505, batch error rate 15.000000%\n",
      "At minibatch 20000, batch loss 1.135911, batch nll 0.726931, batch error rate 25.000000%\n",
      "At minibatch 20100, batch loss 0.753059, batch nll 0.344263, batch error rate 10.000000%\n",
      "At minibatch 20200, batch loss 0.943149, batch nll 0.534571, batch error rate 20.000000%\n",
      "After epoch 9: valid_err_rate: 29.740000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 21.808889% averaged train nll: 0.641327 averaged train loss: 1.052422\n",
      "At minibatch 20300, batch loss 1.108855, batch nll 0.700471, batch error rate 20.000000%\n",
      "At minibatch 20400, batch loss 0.969648, batch nll 0.561446, batch error rate 20.000000%\n",
      "At minibatch 20500, batch loss 0.915784, batch nll 0.507788, batch error rate 15.000000%\n",
      "At minibatch 20600, batch loss 0.902516, batch nll 0.494682, batch error rate 15.000000%\n",
      "At minibatch 20700, batch loss 1.137504, batch nll 0.729868, batch error rate 20.000000%\n",
      "At minibatch 20800, batch loss 1.124692, batch nll 0.717231, batch error rate 15.000000%\n",
      "At minibatch 20900, batch loss 1.077588, batch nll 0.670310, batch error rate 30.000000%\n",
      "At minibatch 21000, batch loss 1.327074, batch nll 0.919975, batch error rate 30.000000%\n",
      "At minibatch 21100, batch loss 1.029779, batch nll 0.622809, batch error rate 25.000000%\n",
      "At minibatch 21200, batch loss 1.030569, batch nll 0.623750, batch error rate 25.000000%\n",
      "At minibatch 21300, batch loss 0.762829, batch nll 0.356195, batch error rate 5.000000%\n",
      "At minibatch 21400, batch loss 0.923409, batch nll 0.516937, batch error rate 10.000000%\n",
      "At minibatch 21500, batch loss 0.874943, batch nll 0.468638, batch error rate 10.000000%\n",
      "At minibatch 21600, batch loss 1.042026, batch nll 0.635990, batch error rate 20.000000%\n",
      "At minibatch 21700, batch loss 1.589900, batch nll 1.183980, batch error rate 40.000000%\n",
      "At minibatch 21800, batch loss 1.182711, batch nll 0.777002, batch error rate 30.000000%\n",
      "At minibatch 21900, batch loss 0.846343, batch nll 0.440824, batch error rate 15.000000%\n",
      "At minibatch 22000, batch loss 1.014724, batch nll 0.609363, batch error rate 25.000000%\n",
      "At minibatch 22100, batch loss 1.137397, batch nll 0.732179, batch error rate 30.000000%\n",
      "At minibatch 22200, batch loss 1.330515, batch nll 0.925520, batch error rate 25.000000%\n",
      "At minibatch 22300, batch loss 0.947017, batch nll 0.542280, batch error rate 20.000000%\n",
      "At minibatch 22400, batch loss 0.866894, batch nll 0.462322, batch error rate 20.000000%\n",
      "At minibatch 22500, batch loss 0.960723, batch nll 0.556285, batch error rate 20.000000%\n",
      "After epoch 10: valid_err_rate: 29.640000% currently going to do 16 epochs\n",
      "After epoch 10: averaged train_err_rate: 20.548889% averaged train nll: 0.607231 averaged train loss: 1.013693\n",
      "At minibatch 22600, batch loss 0.905496, batch nll 0.501088, batch error rate 20.000000%\n",
      "At minibatch 22700, batch loss 1.105060, batch nll 0.700782, batch error rate 20.000000%\n",
      "At minibatch 22800, batch loss 1.016710, batch nll 0.612566, batch error rate 20.000000%\n",
      "At minibatch 22900, batch loss 1.461374, batch nll 1.057329, batch error rate 45.000000%\n",
      "At minibatch 23000, batch loss 1.009398, batch nll 0.605567, batch error rate 30.000000%\n",
      "At minibatch 23100, batch loss 0.687559, batch nll 0.283845, batch error rate 10.000000%\n",
      "At minibatch 23200, batch loss 0.860891, batch nll 0.457258, batch error rate 25.000000%\n",
      "At minibatch 23300, batch loss 1.064816, batch nll 0.661363, batch error rate 30.000000%\n",
      "At minibatch 23400, batch loss 1.279273, batch nll 0.875999, batch error rate 35.000000%\n",
      "At minibatch 23500, batch loss 0.974341, batch nll 0.571205, batch error rate 20.000000%\n",
      "At minibatch 23600, batch loss 0.895119, batch nll 0.492129, batch error rate 25.000000%\n",
      "At minibatch 23700, batch loss 1.298845, batch nll 0.896085, batch error rate 40.000000%\n",
      "At minibatch 23800, batch loss 1.105036, batch nll 0.702415, batch error rate 35.000000%\n",
      "At minibatch 23900, batch loss 0.759785, batch nll 0.357266, batch error rate 10.000000%\n",
      "At minibatch 24000, batch loss 0.852566, batch nll 0.450247, batch error rate 15.000000%\n",
      "At minibatch 24100, batch loss 0.988678, batch nll 0.586495, batch error rate 20.000000%\n",
      "At minibatch 24200, batch loss 0.811530, batch nll 0.409479, batch error rate 10.000000%\n",
      "At minibatch 24300, batch loss 0.712294, batch nll 0.310377, batch error rate 15.000000%\n",
      "At minibatch 24400, batch loss 1.205252, batch nll 0.803547, batch error rate 35.000000%\n",
      "At minibatch 24500, batch loss 0.976118, batch nll 0.574573, batch error rate 25.000000%\n",
      "At minibatch 24600, batch loss 1.657567, batch nll 1.256194, batch error rate 50.000000%\n",
      "At minibatch 24700, batch loss 1.067175, batch nll 0.666036, batch error rate 30.000000%\n",
      "After epoch 11: valid_err_rate: 29.620000% currently going to do 17 epochs\n",
      "After epoch 11: averaged train_err_rate: 19.428889% averaged train nll: 0.578563 averaged train loss: 0.981460\n",
      "At minibatch 24800, batch loss 0.784949, batch nll 0.383963, batch error rate 20.000000%\n",
      "At minibatch 24900, batch loss 0.657390, batch nll 0.256473, batch error rate 5.000000%\n",
      "At minibatch 25000, batch loss 1.058109, batch nll 0.657279, batch error rate 20.000000%\n",
      "At minibatch 25100, batch loss 1.018108, batch nll 0.617375, batch error rate 15.000000%\n",
      "At minibatch 25200, batch loss 0.925329, batch nll 0.524699, batch error rate 15.000000%\n",
      "At minibatch 25300, batch loss 0.975301, batch nll 0.574729, batch error rate 20.000000%\n",
      "At minibatch 25400, batch loss 0.778755, batch nll 0.378344, batch error rate 15.000000%\n",
      "At minibatch 25500, batch loss 1.152352, batch nll 0.752026, batch error rate 30.000000%\n",
      "At minibatch 25600, batch loss 1.075382, batch nll 0.675197, batch error rate 20.000000%\n",
      "At minibatch 25700, batch loss 1.050432, batch nll 0.650346, batch error rate 30.000000%\n",
      "At minibatch 25800, batch loss 1.003934, batch nll 0.603908, batch error rate 25.000000%\n",
      "At minibatch 25900, batch loss 0.731915, batch nll 0.332029, batch error rate 10.000000%\n",
      "At minibatch 26000, batch loss 0.725444, batch nll 0.325651, batch error rate 5.000000%\n",
      "At minibatch 26100, batch loss 0.826278, batch nll 0.426627, batch error rate 15.000000%\n",
      "At minibatch 26200, batch loss 0.780325, batch nll 0.380779, batch error rate 10.000000%\n",
      "At minibatch 26300, batch loss 0.937006, batch nll 0.537639, batch error rate 15.000000%\n",
      "At minibatch 26400, batch loss 1.082395, batch nll 0.683148, batch error rate 25.000000%\n",
      "At minibatch 26500, batch loss 1.207771, batch nll 0.808678, batch error rate 35.000000%\n",
      "At minibatch 26600, batch loss 0.750317, batch nll 0.351311, batch error rate 15.000000%\n",
      "At minibatch 26700, batch loss 0.801370, batch nll 0.402453, batch error rate 20.000000%\n",
      "At minibatch 26800, batch loss 0.978836, batch nll 0.580098, batch error rate 20.000000%\n",
      "At minibatch 26900, batch loss 1.170925, batch nll 0.772351, batch error rate 35.000000%\n",
      "At minibatch 27000, batch loss 0.669878, batch nll 0.271426, batch error rate 5.000000%\n",
      "After epoch 12: valid_err_rate: 29.660000% currently going to do 17 epochs\n",
      "After epoch 12: averaged train_err_rate: 18.340000% averaged train nll: 0.550998 averaged train loss: 0.950856\n",
      "At minibatch 27100, batch loss 0.834444, batch nll 0.436096, batch error rate 15.000000%\n",
      "At minibatch 27200, batch loss 0.675246, batch nll 0.276918, batch error rate 5.000000%\n",
      "At minibatch 27300, batch loss 0.795338, batch nll 0.397058, batch error rate 15.000000%\n",
      "At minibatch 27400, batch loss 1.054209, batch nll 0.656010, batch error rate 25.000000%\n",
      "At minibatch 27500, batch loss 1.200089, batch nll 0.801995, batch error rate 30.000000%\n",
      "At minibatch 27600, batch loss 1.022315, batch nll 0.624285, batch error rate 25.000000%\n",
      "At minibatch 27700, batch loss 1.002794, batch nll 0.604840, batch error rate 25.000000%\n",
      "At minibatch 27800, batch loss 1.023049, batch nll 0.625200, batch error rate 25.000000%\n",
      "At minibatch 27900, batch loss 1.008508, batch nll 0.610745, batch error rate 30.000000%\n",
      "At minibatch 28000, batch loss 0.839554, batch nll 0.441898, batch error rate 15.000000%\n",
      "At minibatch 28100, batch loss 0.839412, batch nll 0.441824, batch error rate 10.000000%\n",
      "At minibatch 28200, batch loss 0.836568, batch nll 0.439101, batch error rate 15.000000%\n",
      "At minibatch 28300, batch loss 0.985783, batch nll 0.588391, batch error rate 20.000000%\n",
      "At minibatch 28400, batch loss 0.959614, batch nll 0.562296, batch error rate 10.000000%\n",
      "At minibatch 28500, batch loss 0.899482, batch nll 0.502286, batch error rate 20.000000%\n",
      "At minibatch 28600, batch loss 1.065129, batch nll 0.668112, batch error rate 25.000000%\n",
      "At minibatch 28700, batch loss 0.880034, batch nll 0.483106, batch error rate 20.000000%\n",
      "At minibatch 28800, batch loss 0.868112, batch nll 0.471292, batch error rate 10.000000%\n",
      "At minibatch 28900, batch loss 0.944432, batch nll 0.547728, batch error rate 15.000000%\n",
      "At minibatch 29000, batch loss 1.120035, batch nll 0.723443, batch error rate 25.000000%\n",
      "At minibatch 29100, batch loss 0.959856, batch nll 0.563384, batch error rate 10.000000%\n",
      "At minibatch 29200, batch loss 1.236343, batch nll 0.839985, batch error rate 20.000000%\n",
      "After epoch 13: valid_err_rate: 29.440000% currently going to do 20 epochs\n",
      "After epoch 13: averaged train_err_rate: 17.304444% averaged train nll: 0.525265 averaged train loss: 0.922754\n",
      "At minibatch 29300, batch loss 0.629554, batch nll 0.233297, batch error rate 0.000000%\n",
      "At minibatch 29400, batch loss 0.959444, batch nll 0.563249, batch error rate 20.000000%\n",
      "At minibatch 29500, batch loss 0.785173, batch nll 0.389024, batch error rate 10.000000%\n",
      "At minibatch 29600, batch loss 0.903997, batch nll 0.507916, batch error rate 15.000000%\n",
      "At minibatch 29700, batch loss 0.732432, batch nll 0.336434, batch error rate 20.000000%\n",
      "At minibatch 29800, batch loss 0.708527, batch nll 0.312622, batch error rate 10.000000%\n",
      "At minibatch 29900, batch loss 0.675654, batch nll 0.279806, batch error rate 10.000000%\n",
      "At minibatch 30000, batch loss 1.004196, batch nll 0.608410, batch error rate 25.000000%\n",
      "At minibatch 30100, batch loss 0.829299, batch nll 0.433560, batch error rate 10.000000%\n",
      "At minibatch 30200, batch loss 0.745157, batch nll 0.349456, batch error rate 10.000000%\n",
      "At minibatch 30300, batch loss 1.139567, batch nll 0.743979, batch error rate 25.000000%\n",
      "At minibatch 30400, batch loss 0.935159, batch nll 0.539629, batch error rate 20.000000%\n",
      "At minibatch 30500, batch loss 0.722761, batch nll 0.327305, batch error rate 10.000000%\n",
      "At minibatch 30600, batch loss 1.009879, batch nll 0.614512, batch error rate 20.000000%\n",
      "At minibatch 30700, batch loss 0.772879, batch nll 0.377557, batch error rate 5.000000%\n",
      "At minibatch 30800, batch loss 0.796779, batch nll 0.401537, batch error rate 15.000000%\n",
      "At minibatch 30900, batch loss 1.177496, batch nll 0.782383, batch error rate 30.000000%\n",
      "At minibatch 31000, batch loss 1.450569, batch nll 1.055574, batch error rate 35.000000%\n",
      "At minibatch 31100, batch loss 0.807961, batch nll 0.413075, batch error rate 15.000000%\n",
      "At minibatch 31200, batch loss 1.100891, batch nll 0.706056, batch error rate 20.000000%"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5\n",
    "# training loop\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in Stream_Train.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        #print [p.get_value().ravel()[:10] for p in model_parameters]\n",
    "        #print [p.get_value().ravel()[:10] for p in velocities]\n",
    "        \n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(Stream_Validation)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
