{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['e']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "\n",
    "from fuel.transformers import ScaleAndShift, Cast\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "\n",
    "from blocks import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generic config\n",
    "\n",
    "Config = {'batchsize-train' : 100,\n",
    "          'batchsize-validation' : 100,\n",
    "          'batchsize-test' : 100,\n",
    "          'theano-debug' : False,\n",
    "         }\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "theano.config.compute_test_value = 'off'\n",
    "if Config['theano-debug']:\n",
    "    print \"Set Theano to debug mode.\"\n",
    "    theano.config.optimizer = 'fast_compile'\n",
    "    theano.config.exception_verbosity = 'high'\n",
    "else:\n",
    "    theano.config.optimizer = 'fast_run'\n",
    "    theano.config.exception_verbosity = 'low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input data.\n",
      "Subset sizes: Train 45000, Validation 5000, Test 10000\n",
      "Stream data shapes:\n",
      "Stream_Train: image batch of shape (100, 3, 32, 32) [float32], and label batch of shape (100, 1) [uint8]\n",
      "Stream_Test: image batch of shape (100, 3, 32, 32) [float32], and label batch of shape (100, 1) [uint8]\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'})\n",
    ")\n",
    "\n",
    "Data_Train      = CIFAR10((\"train\",), subset=slice(None ,45000))\n",
    "Data_Validation = CIFAR10((\"train\",), subset=slice(45000, None))\n",
    "Data_Test       = CIFAR10((\"test\" ,)                           )\n",
    "\n",
    "Stream_Train     = DataStream.default_stream(\n",
    "                      Data_Train,\n",
    "                      iteration_scheme=ShuffledScheme(Data_Train.num_examples       , Config['batchsize-train'])\n",
    "                   )\n",
    "Stream_Validation = DataStream.default_stream(\n",
    "                      Data_Validation,\n",
    "                      iteration_scheme=SequentialScheme(Data_Validation.num_examples, Config['batchsize-validation'])\n",
    "                   )\n",
    "Stream_Test       = DataStream.default_stream(\n",
    "                      Data_Test,\n",
    "                      iteration_scheme=SequentialScheme(Data_Test.num_examples      , Config['batchsize-test'])\n",
    "                   )\n",
    "\n",
    "print \"Loaded input data.\"\n",
    "print \"Subset sizes: Train %d, Validation %d, Test %d\" % (Data_Train.num_examples, Data_Validation.num_examples, Data_Test.num_examples)\n",
    "\n",
    "def GetNextBatch(stream):\n",
    "    return next(stream.get_epoch_iterator())\n",
    "\n",
    "print \"Stream data shapes:\"\n",
    "x,y = GetNextBatch(Stream_Train)\n",
    "print \"Stream_Train: image batch of shape %s [%s], and label batch of shape %s [%s]\" % (x.shape, x.dtype, y.shape, y.dtype)\n",
    "x,y = GetNextBatch(Stream_Test)\n",
    "print \"Stream_Test: image batch of shape %s [%s], and label batch of shape %s [%s]\" % (x.shape, x.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Input_X = theano.tensor.tensor4('X')\n",
    "Input_Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "num_filters_1 = 30 \n",
    "num_filters_2 = 60 \n",
    "num_fw3_hidden = 2000\n",
    "num_fw4_hidden = 10\n",
    "\n",
    "\n",
    "\n",
    "# Layer 1 - convolution 3 -> num_filters_1, filter 5x5, plus bias, then pooling 2x2\n",
    "Param_1_Conv = theano.shared(np.zeros((num_filters_1,3,5,5), dtype='float32'), name='CW1')\n",
    "Param_1_Conv.tag.initializer = IsotropicGaussian(0.05)\n",
    "Param_1_Bias = theano.shared(np.zeros((num_filters_1,), dtype='float32'), name='CB1')\n",
    "Param_1_Bias.tag.initializer = Constant(0.0)\n",
    "\n",
    "# Layer 1 - convolution num_filters_2 -> num_filters_2, filter 5x5, plus bias, then pooling 2x2\n",
    "Param_2_Conv = theano.shared(np.zeros((num_filters_2,num_filters_1,5,5), dtype='float32'), name='CW2')\n",
    "Param_2_Conv.tag.initializer = IsotropicGaussian(0.05)\n",
    "Param_2_Bias = theano.shared(np.zeros((num_filters_2,), dtype='float32'), name='CB2')\n",
    "Param_2_Bias.tag.initializer = Constant(0.0)\n",
    "\n",
    "# Layer 3 - fully connected, num_filters_2 * imagesize -> num_fw3_hidden, plus bias\n",
    "Param_3_W = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype='float32'),name='FW3')\n",
    "Param_3_W.tag.initializer = IsotropicGaussian(0.05)\n",
    "Param_3_Bias = theano.shared(np.zeros((num_fw3_hidden,), dtype='float32'),name='FB3')\n",
    "Param_3_Bias.tag.initializer = Constant(0.0)\n",
    "\n",
    "# Layer 3 - fully connected, num_fw3_hidden -> num_fw4_hidden, plus bias\n",
    "Param_4_W = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype='float32'), name='FW4')\n",
    "Param_4_W.tag.initializer = IsotropicGaussian(0.05)\n",
    "Param_4_Bias = theano.shared(np.zeros((num_fw4_hidden,), dtype='float32'), name='FB4')\n",
    "Param_4_Bias.tag.initializer = Constant(0.0)\n",
    "\n",
    "        \n",
    "ParamList  = []\n",
    "ParamList += [Param_1_Conv, Param_1_Bias]\n",
    "ParamList += [Param_2_Conv, Param_2_Bias]\n",
    "ParamList += [Param_3_W   , Param_3_Bias]\n",
    "ParamList += [Param_4_W   , Param_4_Bias]\n",
    "\n",
    "# Network topology implementation\n",
    "after_C1 = theano.tensor.maximum(    # ReLU\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(Input_X, Param_1_Conv, filter_shape=(5,5)) + Param_1_Bias.dimshuffle('x',0,'x','x')\n",
    ")\n",
    "\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2,2), ignore_border=True)\n",
    "\n",
    "after_C2 = theano.tensor.maximum(    # ReLU\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(after_P1, Param_2_Conv) + Param_2_Bias.dimshuffle('x',0,'x','x')\n",
    ")\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2,2), ignore_border=True)\n",
    "\n",
    "after_F3 = theano.tensor.maximum(    # ReLU\n",
    "    0.0,\n",
    "    theano.tensor.dot(after_P2.flatten(2), Param_3_W) + Param_3_Bias.dimshuffle('x',0)\n",
    ")\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, Param_4_W) + Param_4_Bias.dimshuffle('x',0)\n",
    "\n",
    "\n",
    "Output_Probabilities = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "Output_Predictions   = theano.tensor.argmax(Output_Probabilities, axis=1)\n",
    "\n",
    "Output_ErrRate = theano.tensor.neq(Output_Predictions,Input_Y.ravel()).mean()\n",
    "Output_NLL     = - theano.tensor.log(Output_Probabilities[theano.tensor.arange(Input_Y.shape[0]), Input_Y.ravel()]).mean()\n",
    "\n",
    "Output_WeightDecay = 0.0\n",
    "for p in ParamList:\n",
    "    if p.name[1]=='W':\n",
    "        Output_WeightDecay = Output_WeightDecay + 1e-3 * (p**2).sum()\n",
    "\n",
    "Output_Cost = Output_NLL + Output_WeightDecay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []\n",
    "\n",
    "Input_LRate    = theano.tensor.scalar('lrate',dtype='float32')\n",
    "Input_Momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "Model_Gradients = theano.grad(Output_Cost, ParamList)\n",
    "\n",
    "#initialize storage for momentum\n",
    "Model_Velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in ParamList]\n",
    "\n",
    "for p,g,v in zip(ParamList, Model_Gradients, Model_Velocities):\n",
    "    v_new = Input_Momentum * v - Input_LRate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "Function_TrainStep = theano.function(\n",
    "    [Input_X,Input_Y,Input_LRate,Input_Momentum],\n",
    "    [Output_Cost, Output_ErrRate, Output_NLL, Output_WeightDecay],\n",
    "    updates=updates,\n",
    "    allow_input_downcast=True\n",
    ")\n",
    "#each call to predict will return predictions on a batch of data\n",
    "Function_Predict = theano.function(\n",
    "    [Input_X],\n",
    "    Output_Predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = numpy.random.RandomState(1234)\n",
    "\n",
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (Function_Predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n",
    "\n",
    "def init_parameters():\n",
    "    global rng\n",
    "    for p in ParamList:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in ParamList]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(ParamList, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 8.951997, batch nll 1.536584, batch error rate 50.000000%\n",
      "At minibatch 200, batch loss 8.371844, batch nll 1.330837, batch error rate 46.000000%\n",
      "At minibatch 300, batch loss 8.135177, batch nll 1.455779, batch error rate 50.000000%\n",
      "At minibatch 400, batch loss 7.641599, batch nll 1.305217, batch error rate 49.000000%\n",
      "Time per epoch: 16.4476411343s\n",
      "After epoch 1: valid_err_rate: 43.520000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 54.702222% averaged train nll: 1.522259 averaged train loss: 8.475458\n",
      "At minibatch 500, batch loss 7.180609, batch nll 1.167450, batch error rate 44.000000%\n",
      "At minibatch 600, batch loss 7.043988, batch nll 1.335720, batch error rate 53.000000%\n",
      "At minibatch 700, batch loss 6.630662, batch nll 1.212371, batch error rate 42.000000%\n",
      "At minibatch 800, batch loss 6.104923, batch nll 0.961358, batch error rate 41.000000%\n",
      "At minibatch 900, batch loss 5.981856, batch nll 1.098143, batch error rate 34.000000%\n",
      "Time per epoch: 16.4209120274s\n",
      "After epoch 2: valid_err_rate: 34.460000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 39.704444% averaged train nll: 1.135448 averaged train loss: 6.636283\n",
      "At minibatch 1000, batch loss 5.525653, batch nll 0.884930, batch error rate 27.000000%\n",
      "At minibatch 1100, batch loss 5.408941, batch nll 0.998605, batch error rate 33.000000%\n",
      "At minibatch 1200, batch loss 5.123487, batch nll 0.931839, batch error rate 29.000000%\n",
      "At minibatch 1300, batch loss 4.897168, batch nll 0.912518, batch error rate 30.000000%\n",
      "Time per epoch: 16.4257199764s\n",
      "After epoch 3: valid_err_rate: 32.640000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 32.977778% averaged train nll: 0.951457 averaged train loss: 5.314816\n",
      "At minibatch 1400, batch loss 4.676477, batch nll 0.886352, batch error rate 33.000000%\n",
      "At minibatch 1500, batch loss 4.391562, batch nll 0.783493, batch error rate 25.000000%\n",
      "At minibatch 1600, batch loss 4.126852, batch nll 0.692184, batch error rate 28.000000%\n",
      "At minibatch 1700, batch loss 4.075957, batch nll 0.806319, batch error rate 25.000000%\n",
      "At minibatch 1800, batch loss 3.911138, batch nll 0.797830, batch error rate 29.000000%\n",
      "Time per epoch: 16.4194090366s\n",
      "After epoch 4: valid_err_rate: 31.140000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 27.548889% averaged train nll: 0.794097 averaged train loss: 4.277607\n",
      "At minibatch 1900, batch loss 3.484828, batch nll 0.515373, batch error rate 16.000000%\n",
      "At minibatch 2000, batch loss 3.391053, batch nll 0.558511, batch error rate 20.000000%\n",
      "At minibatch 2100, batch loss 3.510371, batch nll 0.805854, batch error rate 27.000000%\n",
      "At minibatch 2200, batch loss 3.460660, batch nll 0.874042, batch error rate 32.000000%\n",
      "Time per epoch: 16.4243011475s\n",
      "After epoch 5: valid_err_rate: 28.800000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 23.000000% averaged train nll: 0.663869 averaged train loss: 3.470492\n",
      "At minibatch 2300, batch loss 2.965103, batch nll 0.483506, batch error rate 19.000000%\n",
      "At minibatch 2400, batch loss 2.931530, batch nll 0.543931, batch error rate 17.000000%\n",
      "At minibatch 2500, batch loss 2.856803, batch nll 0.555374, batch error rate 17.000000%\n",
      "At minibatch 2600, batch loss 2.696859, batch nll 0.474851, batch error rate 19.000000%\n",
      "At minibatch 2700, batch loss 2.519634, batch nll 0.371359, batch error rate 15.000000%\n",
      "Time per epoch: 16.4269399643s\n",
      "After epoch 6: valid_err_rate: 27.160000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 17.997778% averaged train nll: 0.526250 averaged train loss: 2.854068\n",
      "At minibatch 2800, batch loss 2.428683, batch nll 0.346695, batch error rate 10.000000%\n",
      "At minibatch 2900, batch loss 2.401381, batch nll 0.381119, batch error rate 13.000000%\n",
      "At minibatch 3000, batch loss 2.298080, batch nll 0.336130, batch error rate 16.000000%\n",
      "At minibatch 3100, batch loss 2.388823, batch nll 0.481313, batch error rate 16.000000%\n",
      "Time per epoch: 16.4250879288s\n",
      "After epoch 7: valid_err_rate: 27.440000% currently going to do 10 epochs\n",
      "After epoch 7: averaged train_err_rate: 12.802222% averaged train nll: 0.388667 averaged train loss: 2.396818\n",
      "At minibatch 3200, batch loss 2.114897, batch nll 0.257568, batch error rate 7.000000%\n",
      "At minibatch 3300, batch loss 2.111959, batch nll 0.300425, batch error rate 9.000000%\n",
      "At minibatch 3400, batch loss 2.058326, batch nll 0.290679, batch error rate 10.000000%\n",
      "At minibatch 3500, batch loss 1.997885, batch nll 0.271370, batch error rate 7.000000%\n",
      "At minibatch 3600, batch loss 1.903113, batch nll 0.216156, batch error rate 4.000000%\n",
      "Time per epoch: 16.421544075s\n",
      "After epoch 8: valid_err_rate: 25.120000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 8.788889% averaged train nll: 0.284920 averaged train loss: 2.065083\n",
      "At minibatch 3700, batch loss 1.778335, batch nll 0.128146, batch error rate 5.000000%\n",
      "At minibatch 3800, batch loss 1.737352, batch nll 0.122700, batch error rate 4.000000%\n",
      "At minibatch 3900, batch loss 1.764647, batch nll 0.183994, batch error rate 5.000000%\n",
      "At minibatch 4000, batch loss 1.696122, batch nll 0.147563, batch error rate 4.000000%\n",
      "Time per epoch: 16.4179241657s\n",
      "After epoch 9: valid_err_rate: 25.540000% currently going to do 13 epochs\n",
      "After epoch 9: averaged train_err_rate: 5.335556% averaged train nll: 0.194651 averaged train loss: 1.801826\n",
      "At minibatch 4100, batch loss 1.656700, batch nll 0.138071, batch error rate 3.000000%\n",
      "At minibatch 4200, batch loss 1.590450, batch nll 0.101246, batch error rate 0.000000%\n",
      "At minibatch 4300, batch loss 1.627368, batch nll 0.166514, batch error rate 4.000000%\n",
      "At minibatch 4400, batch loss 1.561221, batch nll 0.126909, batch error rate 2.000000%\n",
      "At minibatch 4500, batch loss 1.571453, batch nll 0.162481, batch error rate 7.000000%\n",
      "Time per epoch: 16.4268848896s\n",
      "After epoch 10: valid_err_rate: 25.620000% currently going to do 13 epochs\n",
      "After epoch 10: averaged train_err_rate: 3.006667% averaged train nll: 0.136702 averaged train loss: 1.605632\n",
      "At minibatch 4600, batch loss 1.465505, batch nll 0.081122, batch error rate 2.000000%\n",
      "At minibatch 4700, batch loss 1.480480, batch nll 0.120250, batch error rate 2.000000%\n",
      "At minibatch 4800, batch loss 1.469812, batch nll 0.132648, batch error rate 1.000000%\n",
      "At minibatch 4900, batch loss 1.404602, batch nll 0.089407, batch error rate 0.000000%\n",
      "Time per epoch: 16.4201960564s\n",
      "After epoch 11: valid_err_rate: 25.500000% currently going to do 13 epochs\n",
      "After epoch 11: averaged train_err_rate: 1.584444% averaged train nll: 0.099593 averaged train loss: 1.454713\n",
      "At minibatch 5000, batch loss 1.409965, batch nll 0.115569, batch error rate 2.000000%\n",
      "At minibatch 5100, batch loss 1.360271, batch nll 0.086719, batch error rate 4.000000%\n",
      "At minibatch 5200, batch loss 1.293099, batch nll 0.039805, batch error rate 0.000000%\n",
      "At minibatch 5300, batch loss 1.288825, batch nll 0.054896, batch error rate 0.000000%\n",
      "At minibatch 5400, batch loss 1.253415, batch nll 0.038099, batch error rate 0.000000%\n",
      "Time per epoch: 16.4201009274s\n",
      "After epoch 12: valid_err_rate: 25.040000% currently going to do 19 epochs\n",
      "After epoch 12: averaged train_err_rate: 0.766667% averaged train nll: 0.072275 averaged train loss: 1.331109\n",
      "At minibatch 5500, batch loss 1.268867, batch nll 0.071895, batch error rate 0.000000%\n",
      "At minibatch 5600, batch loss 1.229838, batch nll 0.050832, batch error rate 0.000000%\n",
      "At minibatch 5700, batch loss 1.222128, batch nll 0.060167, batch error rate 0.000000%\n",
      "At minibatch 5800, batch loss 1.203888, batch nll 0.058197, batch error rate 0.000000%\n",
      "Time per epoch: 16.4162158966s\n",
      "After epoch 13: valid_err_rate: 25.100000% currently going to do 19 epochs\n",
      "After epoch 13: averaged train_err_rate: 0.415556% averaged train nll: 0.059431 averaged train loss: 1.234696\n",
      "At minibatch 5900, batch loss 1.176912, batch nll 0.046814, batch error rate 0.000000%\n",
      "At minibatch 6000, batch loss 1.164046, batch nll 0.049736, batch error rate 0.000000%\n",
      "At minibatch 6100, batch loss 1.153511, batch nll 0.054452, batch error rate 0.000000%\n",
      "At minibatch 6200, batch loss 1.131164, batch nll 0.046627, batch error rate 0.000000%\n",
      "At minibatch 6300, batch loss 1.151086, batch nll 0.080506, batch error rate 0.000000%\n",
      "Time per epoch: 16.4256501198s\n",
      "After epoch 14: valid_err_rate: 24.380000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 0.191111% averaged train nll: 0.049524 averaged train loss: 1.152799\n",
      "At minibatch 6400, batch loss 1.091473, batch nll 0.034611, batch error rate 0.000000%\n",
      "At minibatch 6500, batch loss 1.077143, batch nll 0.033823, batch error rate 0.000000%\n",
      "At minibatch 6600, batch loss 1.065007, batch nll 0.034686, batch error rate 0.000000%\n",
      "At minibatch 6700, batch loss 1.060364, batch nll 0.042477, batch error rate 1.000000%\n",
      "Time per epoch: 16.413433075s\n",
      "After epoch 15: valid_err_rate: 24.840000% currently going to do 22 epochs\n",
      "After epoch 15: averaged train_err_rate: 0.093333% averaged train nll: 0.044308 averaged train loss: 1.084698\n",
      "At minibatch 6800, batch loss 1.038465, batch nll 0.032557, batch error rate 0.000000%\n",
      "At minibatch 6900, batch loss 1.025782, batch nll 0.031828, batch error rate 0.000000%\n",
      "At minibatch 7000, batch loss 1.022822, batch nll 0.040469, batch error rate 0.000000%\n",
      "At minibatch 7100, batch loss 1.013378, batch nll 0.042116, batch error rate 1.000000%\n",
      "At minibatch 7200, batch loss 1.010552, batch nll 0.049834, batch error rate 0.000000%\n",
      "Time per epoch: 16.4177501202s\n",
      "After epoch 16: valid_err_rate: 24.460000% currently going to do 22 epochs\n",
      "After epoch 16: averaged train_err_rate: 0.068889% averaged train nll: 0.042649 averaged train loss: 1.028192\n",
      "At minibatch 7300, batch loss 0.984425, batch nll 0.034326, batch error rate 0.000000%\n",
      "At minibatch 7400, batch loss 0.965649, batch nll 0.025945, batch error rate 0.000000%\n",
      "At minibatch 7500, batch loss 0.974734, batch nll 0.045095, batch error rate 0.000000%\n",
      "At minibatch 7600, batch loss 0.954711, batch nll 0.034764, batch error rate 0.000000%\n",
      "Time per epoch: 16.4191520214s\n",
      "After epoch 17: valid_err_rate: 24.580000% currently going to do 22 epochs\n",
      "After epoch 17: averaged train_err_rate: 0.044444% averaged train nll: 0.039761 averaged train loss: 0.977153\n",
      "At minibatch 7700, batch loss 0.953913, batch nll 0.043316, batch error rate 0.000000%\n",
      "At minibatch 7800, batch loss 0.943252, batch nll 0.041995, batch error rate 0.000000%\n",
      "At minibatch 7900, batch loss 0.923571, batch nll 0.031288, batch error rate 0.000000%\n",
      "At minibatch 8000, batch loss 0.936391, batch nll 0.052821, batch error rate 0.000000%\n",
      "At minibatch 8100, batch loss 0.922716, batch nll 0.047499, batch error rate 0.000000%\n",
      "Time per epoch: 16.4194071293s\n",
      "After epoch 18: valid_err_rate: 24.660000% currently going to do 22 epochs\n",
      "After epoch 18: averaged train_err_rate: 0.035556% averaged train nll: 0.039413 averaged train loss: 0.934122\n",
      "At minibatch 8200, batch loss 0.903495, batch nll 0.036603, batch error rate 0.000000%\n",
      "At minibatch 8300, batch loss 0.887308, batch nll 0.028636, batch error rate 0.000000%\n",
      "At minibatch 8400, batch loss 0.888949, batch nll 0.038277, batch error rate 0.000000%\n",
      "At minibatch 8500, batch loss 0.880024, batch nll 0.036963, batch error rate 0.000000%\n",
      "Time per epoch: 16.4151370525s\n",
      "After epoch 19: valid_err_rate: 24.680000% currently going to do 22 epochs\n",
      "After epoch 19: averaged train_err_rate: 0.022222% averaged train nll: 0.038158 averaged train loss: 0.894990\n",
      "At minibatch 8600, batch loss 0.871759, batch nll 0.036142, batch error rate 0.000000%\n",
      "At minibatch 8700, batch loss 0.867244, batch nll 0.039120, batch error rate 0.000000%\n",
      "At minibatch 8800, batch loss 0.858768, batch nll 0.037869, batch error rate 0.000000%\n",
      "At minibatch 8900, batch loss 0.848472, batch nll 0.034516, batch error rate 0.000000%\n",
      "At minibatch 9000, batch loss 0.846591, batch nll 0.039293, batch error rate 0.000000%\n",
      "Time per epoch: 16.4205169678s\n",
      "After epoch 20: valid_err_rate: 24.600000% currently going to do 22 epochs\n",
      "After epoch 20: averaged train_err_rate: 0.017778% averaged train nll: 0.037983 averaged train loss: 0.860856\n",
      "At minibatch 9100, batch loss 0.834143, batch nll 0.033592, batch error rate 0.000000%\n",
      "At minibatch 9200, batch loss 0.837970, batch nll 0.044026, batch error rate 0.000000%\n",
      "At minibatch 9300, batch loss 0.824903, batch nll 0.037357, batch error rate 0.000000%\n",
      "At minibatch 9400, batch loss 0.836675, batch nll 0.055375, batch error rate 0.000000%\n",
      "Time per epoch: 16.4150319099s\n",
      "After epoch 21: valid_err_rate: 24.780000% currently going to do 22 epochs\n",
      "After epoch 21: averaged train_err_rate: 0.004444% averaged train nll: 0.037056 averaged train loss: 0.829510\n",
      "At minibatch 9500, batch loss 0.802864, batch nll 0.027592, batch error rate 0.000000%\n",
      "At minibatch 9600, batch loss 0.814378, batch nll 0.045231, batch error rate 0.000000%\n",
      "At minibatch 9700, batch loss 0.800926, batch nll 0.037664, batch error rate 0.000000%\n",
      "At minibatch 9800, batch loss 0.797189, batch nll 0.039564, batch error rate 0.000000%\n",
      "At minibatch 9900, batch loss 0.792876, batch nll 0.040712, batch error rate 0.000000%\n",
      "Time per epoch: 16.4165410995s\n",
      "After epoch 22: valid_err_rate: 24.880000% currently going to do 22 epochs\n",
      "After epoch 22: averaged train_err_rate: 0.008889% averaged train nll: 0.037626 averaged train loss: 0.802504\n",
      "TOTAL TIME: 376s\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in Model_Velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "Training_Best_ErrRate = np.inf\n",
    "Training_Best_Params = snapshot_parameters()\n",
    "Training_Best_Epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5\n",
    "# training loop\n",
    "\n",
    "Training_StartTime = time.time()\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    \n",
    "    epoch_starttime = time.time()\n",
    "    for X_batch, Y_batch in Stream_Train.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "        \n",
    "        # momentum = 1.0 - np.maximum(np.minimum(200.0/(i+1), 0.4), 0.01)\n",
    "        momentum=0.97\n",
    "        \n",
    "        L, err_rate, nll, wdec = Function_TrainStep(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "        \n",
    "    epoch_endtime = time.time()\n",
    "    print \"Time per epoch: %ss\" % (epoch_endtime - epoch_starttime)\n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(Stream_Validation)\n",
    "    if val_error_rate < Training_Best_ErrRate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        Training_Best_ErrRate = val_error_rate\n",
    "        Training_Best_Params = snapshot_parameters()\n",
    "        Training_Best_Epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n",
    "\n",
    "Training_EndTime = time.time()\n",
    "print \"TOTAL TIME: %ss\" % int(Training_EndTime - Training_StartTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 1\n",
      "Test error rate is 25.970000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe8d13b7e10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEDCAYAAAA4FgP0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXl4FEX6x79vOOQKkHDfAQUFXBVxOeQwiiIil4siiMil\niIqC68EquoRVF0ERAVFBIYgryMqqHIKASJCfIEEEBAQjZ0g4JQESCDnf3x81Pd0903MlM8xk5v08\nTz/dXV1dXW9fb9Vbb1URM0MQBEGIXKKCnQFBEAQhuIgiEARBiHBEEQiCIEQ4oggEQRAiHFEEgiAI\nEY4oAkEQhAhHFIEgCEKEI4pAEAQhwikbyMSJqDKA9wHkAkhi5kWBvJ4gCILgO4GuEfwNwH+ZeRSA\nPgG+liAIglAMfFYERDSfiE4R0W6H8B5EtJ+I/iCi8bbgBgCO2bYLS5hXQRAEIQAUp0aQCKCHMYCI\nygB4zxbeCsAgImoJIA1AoxJcSxAEQQgwPv+cmXkTgEyH4HYADjDzEWbOB/A5gL4AvgTQn4jeB7C8\npJkVBEEQ/I+/GouNJiBA1QTaM/MlACP8dA1BEAQhAPhLERR7LGsiknGwBUEQigEzkz/S8ZfdPh16\nWwBs22nenszMYbtMnDgx6HkQ+US+SJMtEuTzJ/5SBD8DaE5EcURUHsCD8KFNICEhAUlJSX7KiiAI\nQviSlJSEhIQEv6ZZHPfRxQA2A2hBRMeIaDgzFwAYA2ANgN8ALGHmfd6mmZCQgPj4eF+zIgiCEHHE\nx8f7XRH43EbAzINchK8GsLrEOQozwl3BiXyll3CWDQh/+fwJ+dvW5HMGiHjixImIj4+XBycIguCB\npKQkJCUlYdKkSWA/NRaHhCIIdh4EoTRA5JdvXiiFWP0jichviiCgg855i9ZGIDUCQXCPFJoiD8cC\ngFYj8Os1gv1iSY1AELzDVgIMdjaEK4yr5+7PGoGM/yMIghDhiCIQBKHExMXFYf369QG/TkJCAoYM\nGRLw6xjp2bMnPv30U7+nm5SUhEaN9H64V+oeWhESikA6lAlC6YaIit2YHR8fj3nz5nl9HV+IiorC\noUOHipMtO6tWrboiysfbexgSHcoCgXQoE4TIxZefe3HaSNydU1BQ4HN6wSYQHcpCQhEIglD6SU5O\nRuvWrREbG4sRI0YgNzcXAHDu3Dn06tULtWvXRmxsLHr37o309HQAwIQJE7Bp0yaMGTMG0dHReOaZ\nZwAAe/fuxV133YUaNWqgbt26mDx5MgClNPLy8jB06FBUrVoV119/PbZv326Zn65duwIAbrzxRkRH\nR+OLL75AUlISGjZsiKlTp6JevXoYOXKk2/wB5hrLggUL0LlzZ7zwwguIjY1Fs2bN8O2337q8J3Fx\ncZg2bRpuvPFGVK9eHQMHDrTfl1BCFIEgCCWGmbFo0SKsXbsWBw8eREpKCl5//XUAQFFREUaOHInU\n1FSkpqaiYsWKGDNmDADgjTfeQJcuXTB79mxkZWVh5syZyMrKwp133omePXvixIkTOHDgALp162a/\nzvLlyzFo0CCcP38effr0saflyA8//AAA+PXXX5GVlYUHHngAAHDq1ClkZmYiNTUVc+bMcZs/wNlk\nk5ycjOuuuw5nz57Fiy++iJEjR7q8L0SEL774AmvWrMHhw4fx66+/YsGCBcW/0QEiJBSBtBEIQumG\niDBmzBg0aNAAMTExmDBhAhYvXgwAiI2NxX333YcKFSqgSpUqePnll7Fx40bT+UbzzcqVK1G/fn08\n++yzKF++PKpUqYJ27drZj3fp0gU9evQAEeHhhx/Grl27fMprVFQUJk2ahHLlyqFChQpe5c9IkyZN\nMHLkSBARHnnkEZw4cQKnT592Gf+ZZ55B3bp1ERMTg969e2Pnzp0+5deRQLQRhEyHMkEQSoa/Oh4X\nt6uC0QOmcePGOH78OADg0qVLePbZZ7FmzRpkZqrJDbOzs8HM9pK2scR97NgxNGvWzOV16tSpY9+u\nVKkSLl++jKKiIkRFeVeurVWrFsqXL2/f9yZ/RurWrWu6vha/du3altczxq9YsaL9vhQXrfPtpEmT\nSpSOkZCoEQiCUHKY/bMUl9TUVNN2gwYNAADTpk1DSkoKkpOTcf78eWzcuNE0pr7jz7Zx48YuPX38\nMcyGYxqe8hcJiCIQBKHEMDNmz56N9PR0ZGRk4I033sCDDz4IQJWWK1asiGrVqiEjI8OpJFunTh0c\nPHjQvt+rVy+cOHECM2bMQG5uLrKyspCcnGy/ji84pm2Fp/xFAqIIBEEoMUSEwYMHo3v37rj66qvR\nvHlzvPLKKwCAcePGIScnBzVr1sStt96Ke+65x1QqHzt2LJYuXYrY2FiMGzcOVapUwbp167BixQrU\nq1cPLVq0sLchWvnau6slJCQkYOjQoYiJicHSpUstz/eUP8dr+XJ9T+eHykCCITHWkAxDLQiekbGG\nIhPH5y7DUAtCBCOKIDKRQecEQRCEgCOKQBAEIcIRRSAIghDhiCIQBEGIcEJCEcgQE4IgCN4RiCEm\nxGtIEEoJ4jUUmVwJr6GQGGtIEATvCJUOSEJ4IYpAEEoJUhsQAkVItBEIgiAIwUMUgSAIQoQjikAQ\nBCHCEUUgCIIQ4YgiEARBiHBCQhFIhzJBEATvkA5lgiAIAgAZhloQBEHwI6IIBEEQIhxRBIIgCBGO\nKAJBEIQIRxSBIAhChCOKQBAEIcIRRSAIghDhiCIQBEGIcAKqCIioKRF9TERfBPI6giAIQvEJqCJg\n5sPM/GggryEIgiCUDK8UARHNJ6JTRLTbIbwHEe0noj+IaHxgsigIgiAEEm9rBIkAehgDiKgMgPds\n4a0ADCKilkQ0hIimE1F9/2ZVEARBCAReKQJm3gQg0yG4HYADzHyEmfMBfA6gLzN/yszPMvNxIool\nog8B3CQ1Bv+xfz9w8WKwcyEIQrhQksnrGwA4ZthPA9DeGIGZMwCM9pSQcUjV+Ph4xMfHlyBb4U/L\nlsCYMcCsWcHOiSAIV4qkpKSADdfv9TDURBQHYAUz/8W23x9AD2Z+zLb/MID2zPy0TxmQYah9hggY\nNAhYtKj4aeTmAvPnA0884b98CUI4wgwUFQFlygQ7J2ZCZRjqdACNDPuNoGoFwhUgN7dk52/bBjz5\npH/yIgjhzNy5QNmS2E5KASVRBD8DaE5EcURUHsCDAJYXJyGZocx3SqoICgv9kw9BCHd27Qp2DswE\nYoYyb91HFwPYDKAFER0jouHMXABgDIA1AH4DsISZ9xUnEwkJCdIu4COaIsjMBPbs8f38oiLXx1JT\ngV9/LV6+BCHcCLVCU3x8vN8VgVcVHmYe5CJ8NYDVJc2EpghEGehkZACxsa6Pa4pg9Gjgv/9Vdkxf\n0OIfOwY0amQ+1ru3UgTSdCOEGsxqibqCg+MURxHs3g3ccENgvqFANBqHxFhDL70kNQJHatQA/vzT\n9fG8PLW+cKF46Wsv9513Oh8rqdlJ8J6LF717hrt2AVlZ+n52NrB+feDy5Y6sLCAnJzjXnj37yjfa\nulMEn30GXH+9c/jevYHLTyBqBCGhCF59Ndg5CC00s427vgK5ucCpU8Dhw+7TmjsXOH9ebW/YACxb\nBvTrB3TvrsKsPmjyix9C+NOkCfDDDyVL4667gBYt9P3kZOD3353j3XQTMGGCvj93rlLiRUXApUsl\ny4OvtGgB9OjhOV4g2L3bcxx/U1Cg1mvXAvscjN+ff65++osXq5q0RmmrTYeEIli6NAErVyYFOxt+\nQfvpekthoXOJQ/s5u/vA8/KA22+3/mloHD0KPP448IVtyL/771dK4Mcfzdc3cvFi8Ep7weL4cSA/\n3/fzUlOVci0JBw8qhQ4A06cD7dsDDz+sH09P1xXzrFkqr9p5gCohV65csjz4ysmTwG+/XdlrBhPt\nG7n7buCFF8zHzpxR6//9D1i5Ug83KoKzZ/UavD8IWmNxoDl8OAHz58cHOxsl4sgRVXKoXl2Vur2l\nfXu9JPHww+qjP3dO7RtNAY7k5qqGYnfExan15csqb5pdtVIlPY72km/cqH5qf/mLUiCAerkd+fZb\n99csLfzyC9C2rdpu0AB4803vz922DThwQG07KpC33lLvwsmTwIwZ7tNp0QI4fVrf//vf1drYNmQ8\nDijHgNRU4P331b67goAvfPedb/GLinRlFEokJfm/171WIwCczbXaN2j8Vv71L+Cf/9T3a9YEnnvO\nf/kJW9MQUPr9dJs2BT78UG3v36/WOTnAV1+p7R071E9e+9g1tm8HtmxRJYjPPlNhd9+t1v/4h/lH\nsH27XgK5fNnZVvrDD6qjWWGhqrJqPP000Lq1rgiys/VjhYXqZxIfD9xxh9nUdP/95o+gsBC4554r\nX2NgVo3ajixaBHz/ffHS/P57pQw0HH+47mjXDmjeXG0fOmQ2pb34ouqo98knwLhx7tP54w99e+tW\nfTs9Hfi//3Nt9jN6fP38s1ovXareseJQUKBMVO4KHo5kZADXXFO865UETyaX228HPvrI+lhenl4y\nv3xZ3TNvMNaajYrgwAEgJcUcd/duYOJEXUlq38+hQ95dK1iEjCKIiVEl0XXrgpuPvDxV7V282Pdz\nT5xQa62x9euvgb/9TW0/84xaf/ON83nnzqkSuYbW0LRhgzIHZGern80ttwCTJ+v5dFQE06YpBVC2\nrFIIRlJSrBXBn38C112nto01BQ1jQ6b2Up896xyvOGzebJbbFUlJQOPGzuGDBwOPFnOQc0f32ffe\nUyX4EyeUnd5bNDfbxx/XfxjlypnTP31aPb8lS9S7sX27c2Nihw769t69QJcuyozn+OM7dAi47z59\nX1Mgy5aZTX6+oD1jrSZqxcmT1uHff1+8NqW8PGd7O6Dum7HwURzKlXMOy8wErroK6NZNfSMVKwIP\nPKAKNRs26Epwxw5Voxs4EBg2TClIrTAHmBXBFxazrNxwg3lf88grjunxShIiiiABZ84k4amnVCPm\nRx8Bjzzi/dknTzr70h86VLxG6CFDVOn5oYe8P0d7cbX2gcuX1XrzZj2OVuOpXt06jdtvtw4/dcr8\n446JUeszZ5xLsZr92BXaB+vqpdRMSUa0Wg6gy+nOm8kXOnVSNRFAKRdX98DxesnJQOfOattdfwh3\nWJUsFy5U7rjt2zsf03D86Wml9rlz9R/x5MnAyy/rce65R60HDgQqVFAK3cpby5GsLGcPrieeAHbu\nVNu1aunhFy+a3xNH3nnH9dhUmiJwZ2qsV0+v6Rox1qp8Ye5coFUr5/Bx48xyOaLd/3PnXHu3bdqk\nSuVGtHaYzZvN3lZTpqiacNWqav/mm9XzWrJE1eq++878rpw/D6xapb4/4zN2haZA/akIwraNAEhA\nTEy8/aP++GPg009Vw4yx+uyKgQOVbdvIkiXA66+r7Zwc5w+YyLpEYvWya4wapV4aI9u36yWQ2bPV\nes8e9WN77z21z6wrguRk4NlnVZXenX1XezHz882l5pkz9W1N4Wh4KklpNRZX1WurBsAJE1QJyZi+\nrzWCy5eBnj2tj9Wtq187KUkp79tuU8+msFBVtbXnpH14q1YVv/Srob1r2j0B1DuhPad33gHSvBgw\nxdigf9ttau1oOrP6wZYv7zrN/v3V+vBh984HjorAnW38+edVrTQnx/m90xRBRobr8wHn9w1wX4uw\nYvlyZU83vqvHjunf+Y4dKs3UVP341187K/yYGOCpp9T912piWpwlS5SdXhsjCNCvV1Sk/i8ammkN\n0M2p7v4BAHDvvUDt2p5lNVLSWo6RsG4jmD8fWG3rmqaZPN5+W/+5ukN7EYzmHEctDjg/DK3Bz4iV\neUTjq6/MXiJ3363b842sWAE89pi+n5FhbgN5911V8hkxwvW1NFPBvn1K0WlopWNH0w+R58ZjR556\nyrt4mr1TK9VcvKi2PZUGr7tOKZGDB9WztVJAWg1Juz9r1qi2jlatgAULVFVbK929/baqoRgbrB3T\nPHXK3HCXm6s+/tRUdY+ysoCpU/Vn37SpHnfvXtVeA6jGPceOdsUp/c6YYW3rr1DB9TlaIQBw31fA\n2KhsrBHMnKl+hlZpTpqkmwI1PNUItJ+pVTueK5ORhtHd8667gL59VduTdt6aNUDXrroLrfbtazXF\n7GxlCrNqI9q8WXlMTZmi9h3vc//+6pqA6xK5sbbp+E35E38qgkAQMorAiNH2bSwJ9OsHjB2r7//3\nv8rOp5XIH3rI+oZrJYCLF1UDrNaiHxWlSpaFhUoBnD+vbIeOHDumfgKawtFMPmvXui4db9+ubw8f\n7uw+9uOPZtORI9WqqbWxEdFIvXrOYZq3j0aXLq7TB9wrPat0p05Va60RXPO6AZR86en6/qFDquS5\na5deurNyh9V+CNozr2+Yzsixo1V+vjKNGO+J9n4cPqyW8ePVjyYxUYVXrKjCNNNA1apqf/58tW80\nL1y+bK4hGPORkmKW11tceQ4Ze8Z262Y+pimJpk1VocIVxj6Yly6pn+bhw+ob+de/zHG198nqx60p\nkA8+0Eel1b6jS5f0d9nKBVL7ka5d63yMSCnyM2dUesZClPbz7tHDXJPR3oOsLPW9vfKK2j93zlzC\nB/SaYkqK6tTl2Hj91VeqDePHH801DCP+au/yRKi3EYCZg7oA4JtumsjABtY6kN92G9u3hwxh3r2b\neeNGPey665hff525Vy+1f/fd+rGMDObCQnUcYJ4yRT82c6a+DTDPmKHWN92k1lu3mtM6fpxZjZGt\nluhofZuZmcicnrvFKJM3y7Bh7o8/+6znNNLT1b0zhg0frm//85+e0xg5kvntt833AWCuV0+/30VF\nzOPH6/fFGHfFCuYXXlDb773HJrQ4f/yh7j3A3LevHn7nnZ7z16AB8+XL1seM5z/6qG/33/ic//1v\n93HOnGG+/nrrY82aWYfXqKFvP/aY9bO9917n8ypU0Lezs/XtFi2YBw5k7txZ7ffoYb7XWv4GDFDr\nnBzmdeuY33qL+auvnO/Zjh3qvH/8Qz+2eTPzgQPm/Nx+u7795JPMhw4x5+YyZ2Xp4WlpzEePur5/\ntWur9aRJzN27Wz+z9euZR40q3jN0t1SsaB1ude+Ny1//qtbG/5K7ZcoU9hsbNmzgiRMnsvp9++k/\n7K+Eip0BgFesMN+0fv1c33jjB9a7t9ru1k0Pj4piHj3au5+c4/LVV+oDMoYx69vlyunbxo/Q2+XW\nW72P+/jj7o+PHes5jcuXmVNSzGHz5+vbjj+gPn2Y33lHbR8+rGT817+YJ0xw/bPVlvvv1++X8Z59\n8w3zLbfo+3PmOMfZvp35//5PbRsVsTcLEfPgwb4/C2+X06eZX3rJ9fEZM5QsDRqo/RYtfL/G6NHm\nfe16Q4boYa+9pt+f6tXVdmGhfly7vvF5GOnUSYVr38zzz+txFy92ztOWLUrBO+bN0/LII8ytWpnD\nUlKYN23y7vyePQP3LB2XMmVcH0tIMO+vXGnef/tttXan4Bzvgb/xpyIICdNQhw7mxhej365ml3Q0\ne+Tl6dXrq67Sw4uKVKNjccbLsfK6MZqajNW7du2s03D0VjDSpIn3eTHKNGSI2eZ7+bJ6vVyhtT1c\ndZU5HcDsqdGrl7n9YdkydS1AmVQqV1aNcpmZnr24XPlkz51rbpB7/HHnOG3b6vfW8TlrREdbhzPr\n/S+Ki3GIB0dq19Zddq3Q3IK7dlWmHFc9SK0GSRs+3Hxs/Xolj2ae7N1bNUAXFSmT5hdfKHOH1pPY\nmKaj2c3RhVIzDWkN2W+/rR+zModlZan0jV5j3rBwobPTwVtved8R0ZPnmz/RviErN1DH76thQ327\nfn1lfktN1e+z0aV36FDn9Nw5B4QE/tIoxV0A8LlzzLVqWWtSV+aXWrXMNQejqaRBA/+VLKZO9S3+\nhx9ahz//PPOSJWq7Tx+1/v135sREtd2unVrfcYdaG004R46o0tn//qf2i4qYx4xR27GxepVZM6s8\n84xaMzOfPMn20s9//6vOXb1ahR0+rOIkJDA/9ZTavnRJHcvMVPtffcV8443MMTHeyX/6tFZaMS/v\nvqtv793r+3OwMu24Mo851nQ8LZ5MP66W7t310ll+vrp3jiVzQNVOt29X202b6uFnz6q1VurW0Mya\nf/xhXRK84QY9vrv8ffyxfo72rWjnOi6OJhJvzHLBXGJjrcP/9S/v09BqR6tWWT8z4/7vv6vvd/16\n9dw0Tp9Wx0+cUOawmjWZFy1ibt6cuWxZ/fy0NOtnWRLU7zuMagTlyqnbZYWr8DNnlFuZhnG8laws\n5WJopEGD4uXtxRd9i3/LLeb90bYZm996Sy8VaLWJFi1U93NAL/Fq7pRGuatUUQ1vmp8/kX78nXdU\n4yig7mNKirmxXasRpKaqhnUi3ZNCK81MnKi7umoNldqxu+9WDb7eeiTVrm3dQGpsyPN1OAPA3P8i\nMVHlT+usp5WsNe6917e0b7rJOUwbwkHD8bkCel8GQNVcK1a0bhScNEnP/7XXqjWz7vXj6OAwdChw\n4436u+CIozeTK2bNUmlnZ+vfiqt5JoyeSoDrxlV/Yixlu+K554Crr3YON47rY8STg8T48WrdtKne\nv8PKQaRePaBPH90po0IFYMAA5T5u9NbSvpPKldV3euaM8j5KSTEPQhdqcxo4EhKK4M03E5CXl1Si\nNIydaazMQkaPD+M4IMWhRg19u00btdZG84yNVVXtt95S/sjTp+vVXS2PL72kf/zai6R9iA0aKLfO\nWbN0dzhNydWpo19X854YOlT92F9/XfmKN29u/hlpisBYNdUUhdVwvkSq97PmUeTOzdHVyOFWQysY\nTVTFGR+nShV9u317ZYLRfsSOeezdWykeTbkNGKDW06c7mwFWrFCeK8ZhCfr3d57L+a239G1NeVv9\n9KdPV+6bmzaZwxs0UOYjqwKJ46BxDRuqTmNGmY188oneqcwdu3apd1XrhKiZh6xwVATGPgVW+Vju\nxVyEVh5vRhOw5rllhdYv6O23rfugaG6/ji7Y0dHqOwD0EVI//VT3aGvYEJgzRxVWtN7EN9+s1tp4\nQIsXqz5Dy5bp84JbKQtA/4asPPC0vkv/+Y93Ss9bAtGhzC/VipIssNVxNS+KWbOcq2nXXWfe1xo0\njYsrrxAi5R3x6qt62ObNuinGcfnPfzxXKY2eIL//rqqEmkeS5mlkxZo1yvPIyPLl6jzNFPTyy+bj\nX3+tzDmOaI3JVhhNDVqD4vnz5jie8uoYF2B++GHzfdA8ULxZfvhB365ZU9+eNk3fNnplAWbzxKRJ\nylQCMO/fb87bxx8zL12qx9XQvGG0hv2sLLM8xrS0xsCff1ZeQI7xjI2Cmhnztdc837ennzaH5eQw\n//mnOU5CgnfPwYqdO5UZwptnoHnnWC2aB57VojVOd+2qh/38sznOk086m5Mc7yFgbki28rj5y1+Y\nCwqY4+P1NPLzmS9cYP7uOz2e9kzfe8+clz171AIoc6zj85g1S98/fVo5KWjHtHfRaJLTnBguXLC+\n/3l5rr/D/v1dH/MHtn8n/LGERI0AUKWwe++11pwtW+qNVocPW5c4O3VSmt6Rtm1Vd35jiadWLWsN\n36KFtQkAcF0yrl9flZi0Up27IYG7d3f2jb/nHtWDVfMlZzYf79vXeiwXV6VFwFwN1RoUrcZfcdUA\na8XMmeb+GoA+8JoV06eb9401EmMnHq20Djg3bBvvRWGh9SBnubmqVNi/v7OJR2sYrVxZdWzT7tmw\nYXocTSZtjJi2bfUSv0Z8vBrrSBvXKCpK9ZN4+mnn/DjiaFqsUMFco6xfH+jY0XM6rrjxRuf8jhpl\nHdfdcBzNm6t73Lq18zHNYePTT/UwYw182TJlcjE2WPfpo9Z79+rP+LvvzE4FRpPPggVqXbGiKmUb\n0ypbVr2rxvxrJfCqVc2l8QoV9HfN8Zt9+WXdnAio/0CnTvp+VJR654zvmfZ+uKoRuDNrW01YE6qE\njCKYM0fZ/e64Q6++a142ly6pl3vnTmUnJzIPu9Cunfq4R450Tld7iY0dsBo2dO5dCSiTxbXX6p2P\njKYToznJOJKg9nPRXjp3P2grypZV5oLBg9W+q5fKkUmTXI8J72iyyMtzfpGZfcvrddc5m5I0RWBM\nW/soY2LMyodIKRNjNX/mTPUstC79RLr3xbJl5g9fM6X98IPZy6d8eV1ROio7o0I0emwlJuo/Mk1R\nNmpk7hCn0b693hHqo4/UPahfX5km3JlaAGV28tQ2lZ6umxWLi9YJr6hIfRfGMaOMo90aPcbmzjWn\nccMN6l5oXlBG3nwT+Pe/9fv75pvArbfqx/v0UYrSqJC+/FK/5gcfqDa7bt1UoU5rI2vQQL2HeXm6\nV5r2zK06H95yi/JqO3pUPfNDh9R3Y3z/YmP1fDqaa954w9xh0RErzy7t3SrO6Mj//Kf1sBwhib+q\nFsVd4KLutHUr8759epXUkYwMvTrYrp0ePnSoHn7woN5aX1jIvG2bqkYzKzPBuHEqnmZq0tC8ORo2\n1NMqKFBpvfii7oP9ww/6OQsXlrwaCKgOPCVFM+H4G83kNGiQWn/zjVpPnqybtl55hTk5Wa8yR0Wp\n9U8/qTQmTtTv6erVetqPPqqbxZYtM5sHANVhzRMLF6oOcBq5uXrV3wqAOTXV/fGbbjKHnThh9hoJ\nBf78k/nYMX1fM5VUrqz216xRxydM0O9nZqa+PWOGbn6cM8dsqhk92nwdQO9s1qMH8xtv6MezspSX\nmie0fj+OAMrsyqz3ufGGU6f0/BYVqe8UUCZgb3E0G2ls2RJY805JgB9NQyGrCDRuvdX8smloNsJZ\ns/QXU79B3j28b79V8aZPN8e/cEHtax2hzp0zn6e1BxjROmqVhOho9RMsKWlp5p+sv9B6D2uK4Lff\nzDJrisC4r9n9t2xRYVpbTcuWquezO4w26eee8788CxeqAoIrANUjtzSSk+Ns137ySfPzunhRfUdG\n1qwxK4JTp/Rj58+z3Q5fEn75RblNOwKotkJmZ+XmDu17bd1a7RcUqLY+XwCYZ892DtfaCEIRfyqC\nkJgOJiFBTV5vNYG9q1EmNXty587W7n/e0L27qpo7jomvpX3NNaozlKMJwMoVzDh4WXEp7kT0jjRo\nUHx3WXfQfeHTAAAgAElEQVT885+qaq6N6tqypXmslvh4Z9fNypWVe6dmL9Wq/t5MdWg0DQXC/U7r\nPOeK7dtdDxse6lSo4Gwjf+EF3QQJWHu6dO+uzBk//6y+LaM5ULO9l7RzVJs2uredkb17dVOMsR3F\nE5ppaNcutS5Txiynt1iZhly58AaTpKQkJCUl+TdRf2mU4i4ogboFmHftsg73JdmiInPJRyMrS3mL\nOHL0KPPf/+59+uHGwIHe3d/585X5yMhnn3n/bH75RXXOAZy9b4TAM3So8tjRKChQz0LriBhKWP0H\nfAEwD39SGoAfawSk0gseRMTFzQORakC+8UbncMD7hlfBNwYOVENeFOf+amraqvTlinr11NDdDz7o\n+/UE/0KkGmutZowrzRCpUYqtHE5CFSICMxdjfjhnQsZryJ8Yx1ERQgsi35QAoDroiRIIHYozNWVp\nwNf3Mpwo9aJbTWsXri9qqCA1rcjGl/4npYUqVfQexpFISDQWFxdXP6RI1uxXgqFDrYenEMKfcC0E\naMNNRCqlWhG4IuSHfC3l9Ozpeg5iQRBKH6W6sdgVly+rSeK7dvVrsoIgCCGDPxuLw1IRCIIghDth\n5zWUkJDg/w4SgiAIYUgghqGWGoEgCEIpJOxqBIIgCELwEEUgCIIQ4YgiEARBiHBEEQiCIEQ4oggE\nQRAiHFEEgiAIEY4oAkEQhAhHFIEgCEKEE9BB54ioL4B7AVQFMI+Z1wXyeoIgCILvBLRGwMzLmHkU\ngNEAInJqkXAfOkPkK72Es2xA+MvnT7xSBEQ0n4hOEdFuh/AeRLSfiP4govFukngFwHslyWhpJdxf\nRpGv9BLOsgHhL58/8bZGkAighzGAiMpA/dx7AGgFYBARtSSiIUQ0nYjqk2IKgNXMvNOvORcEQRD8\ngldtBMy8iYjiHILbATjAzEcAgIg+B9CXmd8E8Kkt7BkA3QBUJaJrmHmOn/ItCIIg+AmvRx+1KYIV\nzPwX2/79AO5m5sds+w8DaM/MT/uUASIZelQQBKEY+Gv00ZJ4DfnlB+4vQQRBEITiURKvoXQAjQz7\njQCklSw7giAIwpWmJIrgZwDNiSiOiMpDuYcu90+2BEEQhCuFt+6jiwFsBtCCiI4R0XBmLgAwBsAa\nAL8BWMLM+3y5uA/upyEJETUiog1EtJeI9tgax0FEsUS0johSiGgtEVU3nPOSTd79RNQ9eLn3HiIq\nQ0Q7iGiFbT9s5COi6kS0lIj2EdFvRNQ+zOR7yfZ+7iaiRUR0VWmVz8qNvTiyEFFb2/34g4hmXGk5\nXOFCvrds7+YuIvqSiKoZjvlPPmYOygKgDIADAOIAlAOwE0DLYOWnmDLUBXCTbbsKgN8BtAQwFcCL\ntvDxAN60bbeyyVnOJvcBAFHBlsMLOf8O4DMAy237YSMfgE8AjLBtlwVQLVzks+XxEICrbPtLAAwt\nrfIB6AKgDYDdhjBfZNGcY5IBtLNtrwLQI9iyuZHvLu0ZAHgzUPIFc6whu/spM+cD+BxA3yDmx2eY\n+STb+kcwczaAfQAaAOgD9YOBbd3Ptt0XwGJmzmfldnsA6j6ELETUEEBPAB8D0Br2w0I+W+mqCzPP\nBwBmLmDm8wgT+QBcAJAPoBIRlQVQCcBxlFL5mHkTgEyHYF9kaU9E9QBEM3OyLd5CwzlBxUo+Zl7H\nzEW23a0AGtq2/SpfMBVBAwDHDPtptrBSic29tg3Uw6rDzKdsh04BqGPbrg9zg3ppkHk6gBcAFBnC\nwkW+pgDOEFEiEf1CRB8RUWWEiXzMnAFgGoBUKAVwjtV4X2Ehnw1fZXEMT0foy6gxAqqED/hZvmAq\ngrDpP0BEVQD8D8BYZs4yHmNVP3Mna8jeByLqBeA0M++AXhswUZrlgzIF3QzgfWa+GcBFAP8wRijN\n8hHR1QDGQZkO6gOoYuvvY6c0y+eIF7KUWohoAoA8Zl4UiPSDqQjCwv2UiMpBKYFPmflrW/ApIqpr\nO14PwGlbuKPMDW1hocqtAPoQ0WEAiwHcQUSfInzkSwOQxszbbPtLoRTDyTCR7xYAm5n5LCvnji8B\ndET4yAf49i6m2cIbOoSHtIxENAzKPDvYEOxX+YKpCEq9+ykREYB5AH5j5ncNh5ZDNcrBtv7aED6Q\niMoTUVMAzaEadkISZn6ZmRsxc1MAAwF8z8xDED7ynQRwjIha2ILuBLAXwAqEgXwA9gPoQEQVbe/q\nnVAefuEiH+Dju2h75hds3mEEYIjhnJCDiHpAmWb7MvNlwyH/yhfkVvJ7oDxtDgB4KZh5KWb+O0PZ\nzncC2GFbegCIBfAdgBQAawFUN5zzsk3e/VBDdARdDi9lvQ2611DYyAfgRgDbAOyCKjFXCzP5XoRS\nbruhGlPLlVb5oGqlxwHkQbUvDi+OLADa2u7HAQAzgy2XG/lGAPgDwFHD/+X9QMjn9VhDgiAIQngi\nU1UKgiBEOKIIBEEQIhyPioA8DANBRINt3Z9/JaIfiegGb88VBEEQgo/bNgJSs5D9DuVtkA7VqDaI\nDWMKEVFHKK+Z87YW7gRm7uDNuYIgCELw8VQj8DgMBDNvYdUtHzB3gS71Q0gIgiBEAp4Uga/DQIyE\n3gU6rIaQEARBCFc8zVDmtW8pEd0O5ffayddzBUEQhODhSRF4NQyErYH4I6jhTjN9PFcUhiAIQjFg\nP03168k05HEYCCJqDNUj82FmPuDLuRrB7tEXyGXixIlBz4PIJ/JFmmyRIJ8/cVsjYOYCItJmISsD\nYB4z7yOix23H5wD4J4AYAB+ooS2Qz8ztXJ3r19wLgiAIJcaTaQjMvBrAaoewOYbtRwE86u25giAI\nQmghPYsDTHx8fLCzEFBEvtJLOMsGhL98/iTog84REQc7D4IgCKUNIgL7qbHYo2lIEMIVW5uWIIQ8\ngS4siyIQIhqpjQqhzpUosEgbgSAIQoQjikAQBCHCEUUgCIIQ4YgiEIQQJC4uDuvXrw/4dRISEjBk\nyJCAX8dIz5498emnn17RawruEUUgCCEIERW7kTA+Ph7z5s3z+jq+EBUVhUOHDhUnW3ZWrVp1xZVP\nMFmwYAG6dOkS7Gy4RRSBIIQZvvzci+M15e6cgoICn9MLFIWFhaZ9X8fo8SZ+KMlbEkQRCEKIkpyc\njNatWyM2NhYjRoxAbm4uAODcuXPo1asXateujdjYWPTu3Rvp6ekAgAkTJmDTpk0YM2YMoqOj8cwz\nzwAA9u7di7vuugs1atRA3bp1MXnyZABKaeTl5WHo0KGoWrUqrr/+emzfvt0yP127dgUA3HjjjYiO\njsYXX3yBpKQkNGzYEFOnTkW9evUwcuRIt/kDzDWWBQsWoHPnznjhhRcQGxuLZs2a4dtvv3V5T44f\nP47+/fujdu3aaNasGWbNmmU/lpCQgPvvvx9DhgxBtWrVsGDBAsTHx2PChAno1KkTKleujMOHD2Pz\n5s3461//iurVq6Ndu3bYsmWLKW+vvPKKKb4jcXFxmDp1Km644QZER0ejsLAQb775Jq655hpUrVoV\nrVu3xtdffw0A2LdvH5544gls2bIF0dHRiI2NBQDk5ubi+eefR5MmTVC3bl088cQTuHz5srvXIbCE\nwAh6LAjBIJTfvSZNmvBf/vIXTktL44yMDO7UqRO/8sorzMx89uxZ/vLLLzknJ4ezsrL4gQce4H79\n+tnPjY+P53nz5tn3L1y4wHXr1uV33nmHc3NzOSsri7du3crMzBMnTuQKFSrw6tWruaioiF966SXu\n0KGDy3wRER88eNC+v2HDBi5btiz/4x//4Ly8PM7JyfEpf4mJiVyuXDn++OOPuaioiD/44AOuX7++\n5bULCwv55ptv5tdee43z8/P50KFD3KxZM16zZo1dlnLlyvGyZcuYmTknJ4dvu+02btKkCf/2229c\nWFjIJ0+e5OrVq/N//vMfLiws5MWLF3NMTAxnZGQwMzvFz8/Pt3w2bdq04bS0NL58+TIzM3/xxRd8\n4sQJZmZesmQJV65cmU+ePMnMzAsWLODOnTub0hg3bhz37duXMzMzOSsri3v37s0vvfSSpdyu3lNb\nuH/+w/5KqNgZCOGPUQhvQvndi4uL4zlz5tj3V61axVdffbVl3B07dnBMTIx9Pz4+nj/++GP7/qJF\ni/jmm2+2PHfixIl811132ff37t3LFStWdJkvK0VQvnx5zs3NdXmOVf6MiuCaa66xH7t48SITEZ86\ndcopnZ9++okbN25sCvv3v//Nw4cPt8ty2223mY7Hx8fzxIkT7fsLFy7k9u3bm+J07NiRFyxYYBnf\niri4OE5MTHQb56abbrIrpMTERJMiKCoq4sqVK5vu4+bNm7lp06aWaV0JRSA9iwXBBf7q0FnczsuN\nGunzOjVu3BjHjx8HAFy6dAnPPvss1qxZg8xMNQ9UdnY2mNnePmBsJzh27BiaNWvm8jp16tSxb1eq\nVAmXL19GUVERoqK8sxzXqlUL5cuXt+97kz8jdevWNV1fi1+7dm1TvKNHj+L48eOIiYmxhxUWFtpN\nVgDQsGFDOGK8j8ePH0fjxo1Nx5s0aWK/t47xXeEYZ+HChZg+fTqOHDliz//Zs2ctzz1z5gwuXbqE\ntm3b2sOYGUVFRR6vGyikjUAQXKBqzCVfiktqaqppu0EDNeX3tGnTkJKSguTkZJw/fx4bN2401rCd\nfraNGzd26enjj+ELHNPwlL/i0rhxYzRt2hSZmZn25cKFC1i5cqU9H1byGMMaNGiAo0ePmo4fPXrU\nfm+t5LHCGOfo0aMYNWoUZs+ejYyMDGRmZuL66693+Txq1qyJihUr4rfffrPLce7cOVy4cMGLuxAY\nRBEIQgjCzJg9ezbS09ORkZGBN954Aw8++CAAVdqsWLEiqlWrhoyMDEyaNMl0bp06dXDw4EH7fq9e\nvXDixAnMmDEDubm5yMrKQnJysv06vuCYthWe8ldc2rVrh+joaEydOhU5OTkoLCzEnj178PPPPwNw\nLYsxvGfPnkhJScHixYtRUFCAJUuWYP/+/ejVq5dlfG+4ePEiiAg1a9ZEUVEREhMTsWfPHvvxOnXq\nIC0tDfn5+QCUC+5jjz2GcePG4cyZMwCA9PR0rF271qfr+hNRBIIQghARBg8ejO7du+Pqq69G8+bN\n8corrwAAxo0bh5ycHNSsWRO33nor7rnnHlOpc+zYsVi6dCliY2Mxbtw4VKlSBevWrcOKFStQr149\ntGjRAklJSfbrOJZY3ZWIExISMHToUMTExGDp0qWW53vKn+O1vL1+VFQUVq5ciZ07d6JZs2aoVasW\nRo0aZS9Je1MjiI2NxcqVKzFt2jTUrFkTb7/9NlauXGn35vEkvxWtWrXCc889h44dO6Ju3brYs2cP\nOnfubD/erVs3tG7dGnXr1rWbu6ZMmYJrrrkGHTp0QLVq1XDXXXchJSXFp+v6E4/zERBRDwDvQk03\n+TEzT3E4fh2ARABtAExg5mmGY0cAXABQCNsUlhbpc0mrjIJQHGzjuQc7G4LgFlfv6RWbj4CIygB4\nD8CdANIBbCOi5Wyee/gsgKcB9LNIggHEM3OGPzIrCIIg+B9PpqF2AA4w8xFmzgfwOYC+xgjMfIaZ\nfwaQ7yINmf1DEAQhhPGkCBoAOGbYT7OFeQsD+I6Ifiaix3zNnCAIghB4PPUjKKkBtRMznyCiWgDW\nEdF+Zt5UwjQFQRAEP+JJEaQDMPacaARVK/AKZj5hW58hoq+gTE1OiiAhIcG+HR8fj/j4eG8vIQiC\nEBEkJSXZvb38jVuvISIqC+B3AN0AHAeQDGCQQ2OxFjcBQJbmNURElQCUYeYsIqoMYC2AScy81uE8\n8RoSgoJ4DQmlgaB7DTFzARGNAbAGyn10HjPvI6LHbcfnEFFdANsAVAVQRERjAbQCUBvAlzaf3LIA\nPnNUAoIgCELw8diPIOAZkBqBECSkRiCUBq5EjUB6FgtCGJGUlGQaEO3666/HDz/84FVcX3niiSfw\n+uuvF/t8IXSQ0UcFIYwxjnlTEhYsWIB58+Zh0ybd1+ODDz7wS9rhQlRUFA4cOOB2pNdQRWoEgiCU\nWqyminScotIT3sT3Ns3SamoURSAIIcaUKVPwwAMPmMLGjh2LsWPHAgASExPRqlUrVK1aFVdffTXm\nzp3rMq24uDisX78eAJCTk4Nhw4YhNjYWrVu3xrZt20xxfZ1ucdiwYXj11Vft53/00Udo3rw5atSo\ngb59++LEiRP2Y1FRUZgzZw5atGiBmJgYjBkzxmWemdmel5o1a+LBBx+0z2tw5MgRREVFYf78+WjS\npAm6deuGTz75BJ06dcLf//531KxZE5MmTcKFCxfwyCOPoHbt2oiLi8Mbb7xh/0kvWLDAKb4jjtNe\nfvLJJ9i2bRs6duyImJgY1K9fH08//bR9RFGraTwBYOXKlbjpppsQExODTp06Yffu3S7lDir+muGm\nuAtCeJYoIbwJ1Xfv6NGjXKlSJc7KymJm5oKCAq5Xr559eslvvvmGDx06xMzMGzdu5EqVKvEvv/zC\nzGrGsIYNG9rTiouL4/Xr1zMz8/jx47lr166cmZnJx44d49atW3OjRo3scX2dbnHYsGH86quvMjPz\n+vXruWbNmrxjxw7Ozc3lp59+mrt27WqPS0Tcu3dvPn/+PKempnKtWrX422+/tZT/3Xff5Y4dO3J6\nejrn5eXx448/zoMGDWJm5sOHDzMR8dChQ/nSpUuck5PDiYmJXLZsWX7vvfe4sLCQc3JyeMiQIdyv\nXz/Ozs7mI0eOcIsWLUyzojnGd8Rq2svt27fz1q1bubCwkI8cOcItW7bkd9991ySjcdaxX375hWvX\nrs3JyclcVFTEn3zyCcfFxbmdzc0KV+8pZKpKQSg5ofzude7cmRcuXMjMzGvXrnU5TSUzc79+/XjG\njBnM7F4RGOf3ZWaeO3euKa4j7qZbZDYrghEjRvD48ePtx7Kzs7lcuXJ89OhRZlY/yR9//NF+fMCA\nAfzmm29aXrdly5b2PDMzHz9+nMuVK8eFhYV2RXD48GH78cTERNMUlgUFBVy+fHnet2+fPWzOnDkc\nHx9vGd8Kq2kvHZk+fTrfd9999n1HRTB69Gj7/dG49tpreePGjW7TdeRKKAIxDQmCK4j8sxSDhx56\nCIsXLwYALFq0CIMHD7YfW716NTp06IAaNWogJiYGq1atcjktopHjx487TX9pZOHChWjTpg1iYmIQ\nExODPXv2eJUuAJw4cQJNmjSx71euXBk1atRAenq6PcxxSsrs7GzLtI4cOYL77rvPno9WrVqhbNmy\nOHXqlD2Oo7eTcf/PP/9Efn6+KT+NGzc25cUbbynHaS9TUlLQq1cv1KtXD9WqVcOECRPc3p+jR49i\n2rRpdjliYmKQlpZmMpmFCqIIBMEVHLy5Ku+//34kJSUhPT0dX3/9NR566CEAQG5uLvr3748XX3wR\np0+fRmZmJnr27OlVI2W9evWcpr/U8HW6RUfq169vn68XULN2nT171jQFpLc0btwY3377rWlKykuX\nLqFevXr2OO4ms6lZsybKlStnyk9qaqrpx+5JHqtJbp544gm0atUKBw4cwPnz5/HGG2+4nWe4cePG\nmDBhgkmO7Oxs+0xzoYQoAkEIQWrVqoX4+HgMGzYMzZo1w7XXXgsAyMvLQ15eHmrWrImoqCisXr3a\n6ykOBwwYgMmTJ+PcuXNIS0vDrFmz7Md8nW4R0M3KADBo0CAkJiZi165dyM3Nxcsvv4wOHTo41TqM\n57pi9OjRePnll+2K6syZM1i+fLlXMgJAmTJlMGDAAEyYMAHZ2dk4evQopk+fjocfftjrNKzyl52d\njejoaFSqVAn79+93cp91nMbzsccew4cffojk5GQwMy5evIhvvvnGZU0omIgiEIQQ5aGHHsL69evt\ntQEAiI6OxsyZMzFgwADExsZi8eLF6NvXNEWIy9LuxIkT0aRJEzRt2hQ9evTAI488Yo9bnOkWjaXm\nbt264bXXXkP//v1Rv359HD58GJ9//rnLPLmaVhJQHlJ9+vRB9+7dUbVqVXTs2NE+x7K3ac2aNQuV\nK1dGs2bN0KVLFwwePBjDhw/3eG13ab799ttYtGgRqlatilGjRmHgwIGmOI7TeLZt2xYfffQRxowZ\ng9jYWDRv3hwLFy50e91gIUNMCBGLDDEhlAZkiAlBEAQh4IgiEARBiHBEEQiCIEQ4oggEQRAiHFEE\ngiAIEY5HRUBEPYhoPxH9QUTjLY5fR0RbiOgyET3ny7mCIAhC8PE0Z3EZqDmL74SayH4bHOYsJqJa\nAJoA6Acgk/U5iz2ea4sn7qNCUBD3UaE0EPQ5iwG0A3CAmY/YLvw5gL4A7D9zZj4D4AwR3evruYIQ\nbDx1LBKESMCTImgA4JhhPw1Aey/TLsm5ghBwpDYgCApPbQQl+VLkKxMEQSgFeKoRpAMwjtfaCKpk\n7w1en5uQkGDfjo+PR3x8vJeXEARBiAySkpKQlJQUkLQ9NRaXhWrw7QbgOIBkWDT42uImAMgyNBZ7\nda40FguCIPjOFWssZuYCIhoDYA2AMgDmMfM+InrcdnwOEdWF8giqCqCIiMYCaMXM2Vbn+iPTgiAI\ngv+Q0UcFQRBKITL6qCAIguA3RBEIgiBEOKIIBEEQIhxRBIIgCBGOKAJBEIQIRxSBIAhChCOKQBAE\nIcIRRSAIghDhiCIQBEGIcEQRCIIgRDiiCARBECIcUQSCIAgRjigCQRCECEcUgSAIQoQjikAQBCHC\nEUUgCIIQ4XhUBETUg4j2E9EfRDTeRZyZtuO7iKiNIfwIEf1KRDuIKNmfGRcEQRD8g9upKomoDID3\nANwJNRn9NiJabpxykoh6AriGmZsTUXsAHwDoYDvMAOKZOSMguRcEQRBKjKcaQTsAB5j5CDPnA/gc\nQF+HOH0AfAIAzLwVQHUiqmM47pep1ARBEITA4EkRNABwzLCfZgvzNg4D+I6Ifiaix0qSUUEQBCEw\nuDUNQf3IvcFVqb8zMx8noloA1hHRfmbe5H32BEEQhEDjSRGkA2hk2G8EVeJ3F6ehLQzMfNy2PkNE\nX0GZmpwUQUJCgn07Pj4e8fHxXmVeEAQhUkhKSkJSUlJA0iZm14V+IioL4HcA3QAcB5AMYJBFY/EY\nZu5JRB0AvMvMHYioEoAyzJxFRJUBrAUwiZnXOlyD3eVBEARBcIaIwMx+aYN1WyNg5gIiGgNgDYAy\nAOYx8z4ietx2fA4zryKinkR0AMBFAMNtp9cF8CURadf5zFEJCIIgCMHHbY3gimRAagSCIAg+488a\ngfQsFgRBiHBEEQiCIEQ4oggEQRAiHFEEgiAIEY4oAkEQhAhHFIEgCEKEI4pAEAQhwhFFIAiCEOGI\nIhAEQYhwRBEIgiBEOKIIBEEQIhxRBIIgCBGOKIJSQlaWvn3xIlBUFLhr5ecDOTmBS1+wxviMveHi\nRaCwMDB5EXSKioDsbNfHLl68svkJBKIISgFZWUDVqoA2SGuVKsDkyYG73pAhQKNGnuMJ/qVqVeDc\nOe/jV6kCTJoUuPwIivfeA6KjrY9NnqyeQ2lHFEEp4PJltTaW0nfvDtz1du4Ezp4NXPqCM5qS1561\nt/z6q//zIpg5cMD1sUB+h1cSUQSlAO3nkJnpHCaEB3l55rW3yHsQXMLl/osiKAVoL5vRbBBIGz75\nZaoLwRe0Z+zrjyVcfkShjLvvIVza0jwqAiLqQUT7iegPIhrvIs5M2/FdRNTGl3NVPLV4mqhs2DBg\n/Hjgiy+A668HPv4Y6NQJmDYN6NnTkySKUaOAcePM105P1/dffx3o39/5vAEDlD32k0+Adu1U2JAh\nwP336/n/9Vfg0UeB557Tz+veHZgxwzovRMB99wHPPw8UFOjpaIvWeKi9bEuW6OeuXavH69hRhb3y\nih6msW6d2u/XT79m8+bO8caNU/tt2sBriHTl9N//qmdiRadOnpXL0aMqzp49av3gg0BMjHXc778H\n6tZV8X7/3fv8tmoFPPusLntsrAq/9lq1/9ZbntNwfEZEQPnyap2bq8fbtEk/PnEiMHAgsHWrsjVf\ndRWwY4c6duCAWt9zjzovJwe45holf9WqehqtW+vbGzbo93PjRvU8PTFqlJL966+BFi1UWI0aKp/M\nKr2ffvIsNwA8/LC+v3q1Wnftan3ejBnA3Xe7Tld774cM0cMeeki9ywCQnGy2wf/vf+o53nmnst0b\nufde/d254w7g4EHzezd6tNr//HPg6qvV9vr1av3VV6pd7LnngAce8HwPAPXtr11rPlanjmrAN8bt\n0AGYP9/1PQgJmNnlAjVP8QEAcQDKAdgJoKVDnJ4AVtm22wP4ydtzbfFYvYrMFy+yWwDmmBjmkSPV\n9r33qnXbtmrtDQDzVVeZ97ds0febN7dOC2Bu2pS5f3/9uJZvbfn6a7WuXNl43gbu1Mk5vaIi/bzo\naOYzZ5zTO3JExd26Ve0/+aT1dbX8tGxp3mdmfvVVc5jVeczMZcroYddd5/l+FhaqOP/5zwZmZh4+\n3PU5Wrr5+a7T27BBxVm82Dp/RiZN0o8vW+Y+n475qFbNOX1tu00bq3xtsJTFajl5Uo83ZYoe3qiR\nWs+apYclJqr1qlXmNH76yf01AObJk10/R3eyV6zI/NRTRrk38FtvMV+4oD1L9+cDzAUF5uuOHu0+\nDx07us+f8b03XqtePbU9e7b52GOP6fFvu806j9oyZcoGp3QB5r/9Td/Wvo9Bg9S6fn3n/D7zjDld\n7T129XzOnnV+v/r2dX0Piov6fbv+f/uyeKoRtANwgJmPMHM+gM8B9HWI0wfAJzalshVAdSKq6+W5\nJow28CtBfr55XVKs859kGdfRHc3qXK20rdUIPN0fT6Vuq2qsJruvbogXLqj1Dz8keX2OO48Y7fq+\n5oPZt3jextdISkryOq6r52P1fmnuv5cumcO9MTUcO+Z1ljyQBEDPt6v8G9stzp83HyvpNxvIb37X\nriQAzq7WxvfwzBlzPrwxi3ry7LrS/zF/4EkRNABgfO3SbGHexKnvxbkmrvQN9PQB+Iovrn+O13Sn\nCEoIaMAAAAZKSURBVKzaCDxh9UOxuobjh+0tWlq+2Ejd5V/Lh/Zhanj6cTv+SF2h+XprCqw4GE0/\nVrh6j6zusRaWlmYO98bmf+iQ5zi+oD0XV/k3hjvGycgo2bU9fXvFVeCArsAcC13G9/DIEecwT3jK\nszEtT+9MqFDWw3Fvb79fmhdHj3ZtF9bIzARWrlTb69er9fbtat27t3fXyc1VcbWfSEIC8NFHavuP\nP1yndfgw8Oefro9/+KFaX7xoPv7jj87xjZ2HsrKAsWOd03vhBWVzPHVKrX/6ybWMvXvrLzUA9O2r\nbNH79+thDz/sfN6DDwKVKpnDtHPc3U/th/rddypecrLnc0aMAKpVsz6m/RAd7b69egFRDsWVlBR9\ne/JkYNEi19fUcPVB9uqlb+/Y4Zz/33/X3y9PHj3PPgvUqqW2jS6HmrJ8/309TNt2lPfVV91fAwA2\nb3YO8+bdz8kBli1T25rc8+apdgNAtX9pz9GIscPUiBHmY1u3us/Dli3u86d9T45xTpxQ+5rSu/de\noEwZ/VkAqn3EndybNql1//5AhQp6+L59+rZ2L/fs0a/rmBdHF9GRI12/xwDw9NP6dp8+ah3qDhjE\nblQtEXUAkMDMPWz7LwEoYuYphjgfAkhi5s9t+/sB3AagqadzbeHF0PWCIAgCM/tFxXiqEfwMoDkR\nxQE4DuBBAIMc4iwHMAbA5zbFcY6ZTxHRWS/O9ZsggiAIQvFwqwiYuYCIxgBYA+UFNI+Z9xHR47bj\nc5h5FRH1JKIDAC4CGO7u3EAKIwiCIPiOW9OQIAiCEP4EtWextx3OQhUiakREG4hoLxHtIaJnbOGx\nRLSOiFKIaC0RVTec85JN3v1E1D14ufceIipDRDuIaIVtP2zkI6LqRLSUiPYR0W9E1D7M5HvJ9n7u\nJqJFRHRVaZWPiOYT0Ski2m0I81kWImprux9/EJGL7p5XHhfyvWV7N3cR0ZdEVM1wzH/y+atDgq8L\nvOxwFsoLgLoAbrJtVwHwO4CWAKYCeNEWPh7Am7btVjY5y9nkPgAgKthyeCHn3wF8BmC5bT9s5IPq\nAzPCtl0WQLVwkc+Wx0MArrLtLwEwtLTKB6ALgDYAdhvCfJFFs4AkA2hn214FoEewZXMj313aMwDw\nZqDkC2aNwOcOZ6EGM59k5p227WwA+6D6Stg72dnWtgEe0BfAYmbOZ+YjUA+v3RXNtI8QUUOo3uMf\nQ3cTDgv5bKWrLsw8H1DtWsx8HmEiH4ALAPIBVCKisgAqQTlulEr5mHkTAEcvfl9kaU9E9QBEM7Pm\nKLvQcE5QsZKPmdcxs9YlbiuAhrZtv8oXTEXgTWe1UoPNO6oN1MOqw8ynbIdOAahj264PJadGaZB5\nOoAXABj7Z4aLfE0BnCGiRCL6hYg+IqLKCBP5mDkDwDQAqVAK4Bwzr0OYyGfDV1kcw9MR+jJqjIAq\n4QN+li+YiiBsWqmJqAqA/wEYy8ymeaZY1c/cyRqy94GIegE4zcw74KLTYGmWD8oUdDOA95n5Ziiv\nt38YI5Rm+YjoagDjoEwH9QFUISJTt8LSLJ8jXshSaiGiCQDymNmL7pO+E0xFkA7AOA9WI5g1WamA\niMpBKYFPmdnWRxOnbOMtwVZVO20Ld5S5oS0sVLkVQB8iOgxgMYA7iOhThI98aQDSmHmbbX8plGI4\nGSby3QJgMzOfZeYCAF8C6IjwkQ/w7V1Ms4U3dAgPaRmJaBiUeXawIdiv8gVTEdg7qxFReagOZ8uD\nmB+fISICMA/Ab8z8ruHQcqhGOdjWXxvCBxJReSJqCqA5VMNOSMLMLzNzI2ZuCmAggO+ZeQjCR76T\nAI4RkW1gZtwJYC+AFQgD+QDsB9CBiCra3tU7AfyG8JEP8PFdtD3zCzbvMAIwxHBOyEFEPaBMs32Z\n2TgSlX/lC3Ir+T1QnjYHALwUzLwUM/+doWznOwHssC09AMQC+A5ACoC1AKobznnZJu9+AHcHWwYf\nZL0NutdQ2MgH4EYA2wDsgioxVwsz+V6EUm67oRpTy5VW+aBqpccB5EG1Lw4vjiwA2truxwEAM4Mt\nlxv5RgD4A8BRw//l/UDIJx3KBEEQIhyZqlIQBCHCEUUgCIIQ4YgiEARBiHBEEQiCIEQ4oggEQRAi\nHFEEgiAIEY4oAkEQhAhHFIEgCEKE8/+lJIGj8AImegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8d0dc6650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" %(Training_Best_Epoch)\n",
    "load_parameters(Training_Best_Params)\n",
    "\n",
    "print \"Test error rate is %f%%\" %(compute_error_rate(Stream_Test)*100.0,)\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.2)\n",
    "legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
